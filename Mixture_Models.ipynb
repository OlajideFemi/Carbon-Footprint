{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLLLBkF64IMmhsdQAFqL0S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlajideFemi/Carbon-Footprint/blob/main/Mixture_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "LGsmTc19jKtZ",
        "outputId": "2ceb11ae-26c0-4723-b287-da4c90b9695b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3667588727.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3667588727.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    init pi = np.ones(H)/H\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Inputs: V (N x D with values {0,1} and np.nan for missing), H=2\n",
        "# Outputs: pi (H,), theta (H x D), responsibilities R (N x H)\n",
        "\n",
        "init pi = np.ones(H)/H\n",
        "init theta = rng.uniform(0.25, 0.75, size=(H, D))\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # E-step\n",
        "    logR = np.log(pi)[None, :]  # (1 x H) broadcast to (N x H)\n",
        "    for k in range(H):\n",
        "        # add sum over observed dims: v*log(theta) + (1-v)*log(1-theta)\n",
        "        term = 0\n",
        "        for d in range(D):\n",
        "            obs = ~np.isnan(V[:, d])\n",
        "            v = V[obs, d]\n",
        "            loglik = v*np.log(theta[k, d]) + (1 - v)*np.log(1 - theta[k, d])\n",
        "            logR[obs, k] += loglik\n",
        "    # normalize\n",
        "    R = softmax(logR, axis=1)\n",
        "\n",
        "    # M-step\n",
        "    Nk = R.sum(axis=0)              # (H,)\n",
        "    pi = Nk / N\n",
        "    for k in range(H):\n",
        "        for d in range(D):\n",
        "            obs = ~np.isnan(V[:, d])\n",
        "            num = (R[obs, k] * V[obs, d]).sum()\n",
        "            den = (R[obs, k]).sum()\n",
        "            if den > 0:\n",
        "                theta[k, d] = num / den\n",
        "            # else: keep theta[k,d] or apply prior\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DOixZ9SsIb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHVK3IQosIfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VaLJr_l9sIiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjaDUpwNsIm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u4-fm1FDsIps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Parameters\n",
        "num_respondents = 150\n",
        "num_questions = 5\n",
        "missing_percentage = 0.30\n",
        "\n",
        "# Generate a base dataset with 'yes' (1) and 'no' (0) responses\n",
        "# We use a random binomial distribution for this example\n",
        "data = np.random.randint(2, size=(num_respondents, num_questions))\n",
        "df = pd.DataFrame(data, columns=[f'Q{i+1}' for i in range(num_questions)])\n",
        "\n",
        "# Convert numerical responses to 'Yes' and 'No' for clarity\n",
        "df = df.replace({1: 'Yes', 0: 'No'})\n",
        "\n",
        "# Introduce missing data\n",
        "missing_mask = np.random.rand(num_respondents, num_questions) < missing_percentage\n",
        "df_missing = df.mask(missing_mask, pd.NA)\n",
        "\n",
        "# Add a 'Respondent_ID' column\n",
        "df_missing.insert(0, 'Respondent_ID', range(1, num_respondents + 1))\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_filename = 'questionnaire_data.csv'\n",
        "df_missing.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Dataset generated and saved as '{output_filename}'\")\n",
        "print(\"\\nFirst 5 rows of the generated dataset:\")\n",
        "print(df_missing.head())\n",
        "print(f\"\\nTotal missing values: {df_missing.isnull().sum().sum()}\")\n",
        "print(f\"Total possible values: {num_respondents * num_questions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2jwEbpKjZ1j",
        "outputId": "c036971e-5901-43ee-8fd7-205d3fe65c3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset generated and saved as 'questionnaire_data.csv'\n",
            "\n",
            "First 5 rows of the generated dataset:\n",
            "   Respondent_ID    Q1    Q2    Q3   Q4    Q5\n",
            "0              1    No   Yes  <NA>   No  <NA>\n",
            "1              2  <NA>    No    No   No   Yes\n",
            "2              3    No  <NA>  <NA>   No   Yes\n",
            "3              4    No   Yes   Yes  Yes    No\n",
            "4              5   Yes    No   Yes  Yes   Yes\n",
            "\n",
            "Total missing values: 237\n",
            "Total possible values: 750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_missing.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-jiDDh_jZ4k",
        "outputId": "5cec44b7-8cdc-47ee-81a6-be8956eb1530"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzIZQlQujZ7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- 1. Load and preprocess the data ---\n",
        "file_path = 'questionnaire_data.csv'\n",
        "df = pd.read_csv(file_path, index_col='Respondent_ID')\n",
        "\n",
        "# Convert 'Yes'/'No' to 1/0 and keep missing values as np.nan\n",
        "V = df.replace({'Yes': 1, 'No': 0}).values.astype(float)\n",
        "N, D = V.shape\n",
        "H = 2  # Number of components/clusters\n",
        "\n",
        "print(\"Data loaded and preprocessed.\")\n",
        "print(f\"Dataset shape: {V.shape}\")\n",
        "print(f\"Number of respondents (N): {N}\")\n",
        "print(f\"Number of questions (D): {D}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 2. EM Algorithm Implementation ---\n",
        "\n",
        "def run_em(V, H, max_iters=100, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Runs the EM algorithm for a product-of-Bernoulli mixture.\n",
        "    V: data matrix (N x D) with 1/0 and np.nan.\n",
        "    H: number of components.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    pi = np.ones(H) / H\n",
        "    theta = np.random.uniform(0.25, 0.75, size=(H, D))\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # E-step: Compute responsibilities (rnk)\n",
        "        log_R = np.log(pi)[None, :]  # (1, H) broadcast to (N, H)\n",
        "        for k in range(H):\n",
        "            for d in range(D):\n",
        "                # Identify observed data for question d\n",
        "                observed_mask = ~np.isnan(V[:, d])\n",
        "                v_obs = V[observed_mask, d]\n",
        "\n",
        "                # Calculate log-likelihood term for component k and question d\n",
        "                log_theta_k_d = np.log(np.clip(theta[k, d], 1e-6, 1 - 1e-6))\n",
        "                log_1_minus_theta_k_d = np.log(np.clip(1 - theta[k, d], 1e-6, 1 - 1e-6))\n",
        "\n",
        "                log_lik_term = v_obs * log_theta_k_d + (1 - v_obs) * log_1_minus_theta_k_d\n",
        "\n",
        "                # Add term to the log-responsibility for observed data points\n",
        "                log_R[observed_mask, k] += log_lik_term\n",
        "\n",
        "        # Normalize log responsibilities using log-sum-exp trick for stability\n",
        "        log_R -= np.logaddexp.reduce(log_R, axis=1)[:, np.newaxis]\n",
        "        R = np.exp(log_R)\n",
        "\n",
        "        # M-step: Update parameters (pi, theta)\n",
        "        sum_R = R.sum(axis=0)  # sum over N (H,)\n",
        "        pi = sum_R / N\n",
        "\n",
        "        for k in range(H):\n",
        "            for d in range(D):\n",
        "                observed_mask = ~np.isnan(V[:, d])\n",
        "\n",
        "                # Numerator: sum(r_nk * v_nd) for observed data\n",
        "                numerator = (R[observed_mask, k] * V[observed_mask, d]).sum()\n",
        "\n",
        "                # Denominator: sum(r_nk) for observed data\n",
        "                denominator = R[observed_mask, k].sum()\n",
        "\n",
        "                if denominator > 0:\n",
        "                    theta[k, d] = numerator / denominator\n",
        "                # else: keep previous theta or apply prior (not implemented here)\n",
        "\n",
        "        # Calculate incomplete log-likelihood for convergence check\n",
        "        log_likelihood = np.sum(np.log(np.sum(np.exp(log_R), axis=1)))\n",
        "        log_likelihoods.append(log_likelihood)\n",
        "\n",
        "        # Check for convergence\n",
        "        if i > 1 and np.abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
        "            print(f\"EM converged at iteration {i+1}.\")\n",
        "            break\n",
        "\n",
        "    return pi, theta, R, log_likelihoods\n",
        "\n",
        "# Run the EM algorithm\n",
        "pi_final, theta_final, R_final, log_likelihoods = run_em(V, H)\n",
        "\n",
        "# --- 3. Analysis and Visualization ---\n",
        "\n",
        "print(\"\\n--- EM Results ---\")\n",
        "print(\"Final Mixture Weights (π):\", np.round(pi_final, 4))\n",
        "print(\"Final Bernoulli Probabilities (θ):\\n\", np.round(theta_final, 4))\n",
        "print(f\"Total iterations: {len(log_likelihoods)}\")\n",
        "\n",
        "# Plotting the posterior probabilities (like Fig 20.4/20.5)\n",
        "# Sort respondents based on their posterior probability for one cluster (e.g., cluster 2)\n",
        "p_h2_given_v = R_final[:, 1]\n",
        "sorted_indices = np.argsort(p_h2_given_v)\n",
        "sorted_posteriors = p_h2_given_v[sorted_indices]\n",
        "sorted_respondent_ids = df.index[sorted_indices]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_posteriors, marker='.', linestyle='none', color='skyblue', label='Posterior P(h=2 | v)')\n",
        "plt.title('Posterior Probability of Cluster 2 Membership for Each Respondent')\n",
        "plt.xlabel('Respondent (sorted by posterior probability)')\n",
        "plt.ylabel('Posterior Probability')\n",
        "plt.grid(True, linestyle='--')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Assign hard labels\n",
        "hard_labels = R_final.argmax(axis=1)\n",
        "print(f\"\\nExample of hard cluster assignments (first 10 respondents):\\n{hard_labels[:10]}\")\n",
        "\n",
        "# You can now use these hard labels for further analysis or classification.\n",
        "# For example, to find the number of respondents in each cluster:\n",
        "cluster_counts = pd.Series(hard_labels).value_counts().sort_index()\n",
        "print(\"\\nNumber of respondents in each cluster:\")\n",
        "print(cluster_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "pJkDI2-tjZ9r",
        "outputId": "c914aec5-cc54-40cb-ace5-ea554cd632a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and preprocessed.\n",
            "Dataset shape: (150, 5)\n",
            "Number of respondents (N): 150\n",
            "Number of questions (D): 5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-823959534.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  V = df.replace({'Yes': 1, 'No': 0}).values.astype(float)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "boolean index did not match indexed array along axis 0; size of axis is 1 but size of corresponding boolean axis is 150",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-823959534.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Run the EM algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mpi_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# --- 3. Analysis and Visualization ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-823959534.py\u001b[0m in \u001b[0;36mrun_em\u001b[0;34m(V, H, max_iters, tol)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# Add term to the log-responsibility for observed data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mlog_R\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobserved_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog_lik_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Normalize log responsibilities using log-sum-exp trick for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along axis 0; size of axis is 1 but size of corresponding boolean axis is 150"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Load and preprocess the data ---\n",
        "file_path = 'questionnaire_data.csv'\n",
        "df = pd.read_csv(file_path, index_col='Respondent_ID')\n",
        "\n",
        "# Convert 'Yes'/'No' to 1/0 and keep missing values as np.nan\n",
        "V = df.replace({'Yes': 1, 'No': 0}).values.astype(float)\n",
        "N, D = V.shape\n",
        "H = 2  # Number of components/clusters\n",
        "\n",
        "print(\"Data loaded and preprocessed.\")\n",
        "print(f\"Dataset shape: {V.shape}\")\n",
        "print(f\"Number of respondents (N): {N}\")\n",
        "print(f\"Number of questions (D): {D}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 2. Corrected EM Algorithm Implementation ---\n",
        "\n",
        "def run_em(V, H, max_iters=100, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Runs the EM algorithm for a product-of-Bernoulli mixture.\n",
        "    V: data matrix (N x D) with 1/0 and np.nan.\n",
        "    H: number of components.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    pi = np.ones(H) / H\n",
        "    theta = np.random.uniform(0.25, 0.75, size=(H, D))\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # E-step: Compute responsibilities (rnk)\n",
        "        log_R = np.log(pi)[None, :]  # (1, H) broadcast to (N, H)\n",
        "        for k in range(H):\n",
        "            for d in range(D):\n",
        "                # Identify observed data for question d\n",
        "                observed_mask = ~np.isnan(V[:, d])\n",
        "                v_obs = V[observed_mask, d]\n",
        "\n",
        "                # Calculate log-likelihood term for component k and question d\n",
        "                log_theta_k_d = np.log(np.clip(theta[k, d], 1e-6, 1 - 1e-6))\n",
        "                log_1_minus_theta_k_d = np.log(np.clip(1 - theta[k, d], 1e-6, 1 - 1e-6))\n",
        "\n",
        "                log_lik_term = v_obs * log_theta_k_d + (1 - v_obs) * log_1_minus_theta_k_d\n",
        "\n",
        "                # CORRECTED LINE: Add term to the log-responsibility only for observed data points\n",
        "                log_R[observed_mask, k] += log_lik_term\n",
        "\n",
        "        # Normalize log responsibilities using log-sum-exp trick for stability\n",
        "        log_R -= np.logaddexp.reduce(log_R, axis=1)[:, np.newaxis]\n",
        "        R = np.exp(log_R)\n",
        "\n",
        "        # M-step: Update parameters (pi, theta)\n",
        "        sum_R = R.sum(axis=0)  # sum over N (H,)\n",
        "        pi = sum_R / N\n",
        "\n",
        "        for k in range(H):\n",
        "            for d in range(D):\n",
        "                observed_mask = ~np.isnan(V[:, d])\n",
        "\n",
        "                # Numerator: sum(r_nk * v_nd) for observed data\n",
        "                numerator = (R[observed_mask, k] * V[observed_mask, d]).sum()\n",
        "\n",
        "                # Denominator: sum(r_nk) for observed data\n",
        "                denominator = R[observed_mask, k].sum()\n",
        "\n",
        "                if denominator > 0:\n",
        "                    theta[k, d] = numerator / denominator\n",
        "                # else: keep previous theta or apply prior (not implemented here)\n",
        "\n",
        "        # Calculate incomplete log-likelihood for convergence check\n",
        "        log_likelihood = np.sum(np.log(np.sum(np.exp(log_R), axis=1)))\n",
        "        log_likelihoods.append(log_likelihood)\n",
        "\n",
        "        # Check for convergence\n",
        "        if i > 1 and np.abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
        "            print(f\"EM converged at iteration {i+1}.\")\n",
        "            break\n",
        "\n",
        "    return pi, theta, R, log_likelihoods\n",
        "\n",
        "# Run the EM algorithm\n",
        "pi_final, theta_final, R_final, log_likelihoods = run_em(V, H)\n",
        "\n",
        "# --- 3. Analysis and Visualization ---\n",
        "\n",
        "print(\"\\n--- EM Results ---\")\n",
        "print(\"Final Mixture Weights (π):\", np.round(pi_final, 4))\n",
        "print(\"Final Bernoulli Probabilities (θ):\\n\", np.round(theta_final, 4))\n",
        "print(f\"Total iterations: {len(log_likelihoods)}\")\n",
        "\n",
        "# Plotting the posterior probabilities (like Fig 20.4/20.5)\n",
        "p_h2_given_v = R_final[:, 1]\n",
        "sorted_indices = np.argsort(p_h2_given_v)\n",
        "sorted_posteriors = p_h2_given_v[sorted_indices]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_posteriors, marker='.', linestyle='none', color='skyblue', label='Posterior P(h=2 | v)')\n",
        "plt.title('Posterior Probability of Cluster 2 Membership for Each Respondent')\n",
        "plt.xlabel('Respondent (sorted by posterior probability)')\n",
        "plt.ylabel('Posterior Probability')\n",
        "plt.grid(True, linestyle='--')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Assign hard labels\n",
        "hard_labels = R_final.argmax(axis=1)\n",
        "print(f\"\\nExample of hard cluster assignments (first 10 respondents):\\n{hard_labels[:10]}\")\n",
        "\n",
        "# You can now use these hard labels for further analysis or classification.\n",
        "cluster_counts = pd.Series(hard_labels).value_counts().sort_index()\n",
        "print(\"\\nNumber of respondents in each cluster:\")\n",
        "print(cluster_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "7jGSNmwZsKGY",
        "outputId": "50228904-7962-4e34-dc45-18b8b1e3d11a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and preprocessed.\n",
            "Dataset shape: (150, 5)\n",
            "Number of respondents (N): 150\n",
            "Number of questions (D): 5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-240336049.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  V = df.replace({'Yes': 1, 'No': 0}).values.astype(float)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "boolean index did not match indexed array along axis 0; size of axis is 1 but size of corresponding boolean axis is 150",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-240336049.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Run the EM algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mpi_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# --- 3. Analysis and Visualization ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-240336049.py\u001b[0m in \u001b[0;36mrun_em\u001b[0;34m(V, H, max_iters, tol)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# CORRECTED LINE: Add term to the log-responsibility only for observed data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mlog_R\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobserved_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog_lik_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Normalize log responsibilities using log-sum-exp trick for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along axis 0; size of axis is 1 but size of corresponding boolean axis is 150"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Load and preprocess the data ---\n",
        "file_path = 'questionnaire_data.csv'\n",
        "df = pd.read_csv(file_path, index_col='Respondent_ID')\n",
        "\n",
        "# Convert 'Yes'/'No' to 1/0 and keep missing values as np.nan\n",
        "V = df.replace({'Yes': 1, 'No': 0}).values.astype(float)\n",
        "N, D = V.shape\n",
        "H = 2  # Number of components/clusters\n",
        "\n",
        "print(\"Data loaded and preprocessed.\")\n",
        "print(f\"Dataset shape: {V.shape}\")\n",
        "print(f\"Number of respondents (N): {N}\")\n",
        "print(f\"Number of questions (D): {D}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 2. Corrected EM Algorithm Implementation ---\n",
        "\n",
        "def run_em(V, H, max_iters=100, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Runs the EM algorithm for a product-of-Bernoulli mixture.\n",
        "    V: data matrix (N x D) with 1/0 and np.nan.\n",
        "    H: number of components.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    pi = np.ones(H) / H\n",
        "    theta = np.random.uniform(0.25, 0.75, size=(H, D))\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # E-step: Compute responsibilities (rnk)\n",
        "        # CORRECTED: Initialize log_R with full N x H shape here.\n",
        "        log_R = np.log(pi)[None, :]  # (1, H) broadcast to (N, H)\n",
        "\n",
        "        for k in range(H):\n",
        "            for d in range(D):\n",
        "                # Identify observed data for question d\n",
        "                observed_mask = ~np.isnan(V[:, d])\n",
        "                v_obs = V[observed_mask, d]\n",
        "\n",
        "                # Calculate log-likelihood term for component k and question d\n",
        "                log_theta_k_d = np.log(np.clip(theta[k, d], 1e-6, 1 - 1e-6))\n",
        "                log_1_minus_theta_k_d = np.log(np.clip(1 - theta[k, d], 1e-6, 1 - 1e-6))\n",
        "                log_lik_term = v_obs * log_theta_k_d + (1 - v_obs) * log_1_minus_theta_k_d\n",
        "\n",
        "                # Add term to the log-responsibility only for observed data points\n",
        "                log_R[observed_mask, k] += log_lik_term\n",
        "\n",
        "        # Normalize log responsibilities using log-sum-exp trick for stability\n",
        "        log_R -= np.logaddexp.reduce(log_R, axis=1)[:, np.newaxis]\n",
        "        R = np.exp(log_R)\n",
        "\n",
        "        # M-step: Update parameters (pi, theta)\n",
        "        sum_R = R.sum(axis=0)  # sum over N (H,)\n",
        "        pi = sum_R / N\n",
        "\n",
        "        for k in range(H):\n",
        "            for d in range(D):\n",
        "                observed_mask = ~np.isnan(V[:, d])\n",
        "\n",
        "                # Numerator: sum(r_nk * v_nd) for observed data\n",
        "                numerator = (R[observed_mask, k] * V[observed_mask, d]).sum()\n",
        "\n",
        "                # Denominator: sum(r_nk) for observed data\n",
        "                denominator = R[observed_mask, k].sum()\n",
        "\n",
        "                if denominator > 0:\n",
        "                    theta[k, d] = numerator / denominator\n",
        "                # else: keep previous theta or apply prior (not implemented here)\n",
        "\n",
        "        # Calculate incomplete log-likelihood for convergence check\n",
        "        log_likelihood = np.sum(np.log(np.sum(np.exp(log_R), axis=1)))\n",
        "        log_likelihoods.append(log_likelihood)\n",
        "\n",
        "        # Check for convergence\n",
        "        if i > 1 and np.abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
        "            print(f\"EM converged at iteration {i+1}.\")\n",
        "            break\n",
        "\n",
        "    return pi, theta, R, log_likelihoods\n",
        "\n",
        "# Run the EM algorithm\n",
        "pi_final, theta_final, R_final, log_likelihoods = run_em(V, H)\n",
        "\n",
        "# --- 3. Analysis and Visualization ---\n",
        "\n",
        "print(\"\\n--- EM Results ---\")\n",
        "print(\"Final Mixture Weights (π):\", np.round(pi_final, 4))\n",
        "print(\"Final Bernoulli Probabilities (θ):\\n\", np.round(theta_final, 4))\n",
        "print(f\"Total iterations: {len(log_likelihoods)}\")\n",
        "\n",
        "# Plotting the posterior probabilities (like Fig 20.4/20.5)\n",
        "p_h2_given_v = R_final[:, 1]\n",
        "sorted_indices = np.argsort(p_h2_given_v)\n",
        "sorted_posteriors = p_h2_given_v[sorted_indices]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_posteriors, marker='.', linestyle='none', color='skyblue', label='Posterior P(h=2 | v)')\n",
        "plt.title('Posterior Probability of Cluster 2 Membership for Each Respondent')\n",
        "plt.xlabel('Respondent (sorted by posterior probability)')\n",
        "plt.ylabel('Posterior Probability')\n",
        "plt.grid(True, linestyle='--')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Assign hard labels\n",
        "hard_labels = R_final.argmax(axis=1)\n",
        "print(f\"\\nExample of hard cluster assignments (first 10 respondents):\\n{hard_labels[:10]}\")\n",
        "\n",
        "# You can now use these hard labels for further analysis or classification.\n",
        "cluster_counts = pd.Series(hard_labels).value_counts().sort_index()\n",
        "print(\"\\nNumber of respondents in each cluster:\")\n",
        "print(cluster_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "U_yvytPSsKJb",
        "outputId": "fe5fbf01-3be4-450e-dd8a-881d1629e44a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and preprocessed.\n",
            "Dataset shape: (150, 5)\n",
            "Number of respondents (N): 150\n",
            "Number of questions (D): 5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-340706470.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  V = df.replace({'Yes': 1, 'No': 0}).values.astype(float)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "boolean index did not match indexed array along axis 0; size of axis is 1 but size of corresponding boolean axis is 150",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-340706470.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Run the EM algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mpi_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# --- 3. Analysis and Visualization ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-340706470.py\u001b[0m in \u001b[0;36mrun_em\u001b[0;34m(V, H, max_iters, tol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# Add term to the log-responsibility only for observed data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mlog_R\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobserved_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog_lik_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Normalize log responsibilities using log-sum-exp trick for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along axis 0; size of axis is 1 but size of corresponding boolean axis is 150"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mo09aiyesKQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XtcqawljsKVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPJykUnhsKYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9tcgSmvsKa4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
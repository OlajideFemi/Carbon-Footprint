{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlajideFemi/Carbon-Footprint/blob/main/Queueing_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qRdyPJEwKoH",
        "outputId": "2deb32f4-c13b-476d-8edf-a9b4e093b023"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_port_daily_data\u001b[39m(port_ids=\u001b[38;5;28;01mNone\u001b[39;00m, countries=\u001b[38;5;28;01mNone\u001b[39;00m, start_date=\u001b[33m\"\u001b[39m\u001b[33m2024-04-01\u001b[39m\u001b[33m\"\u001b[39m, end_date=\u001b[33m\"\u001b[39m\u001b[33m2026-03-31\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_port_daily_data(port_ids=None, countries=None, start_date=\"2024-04-01\", end_date=\"2026-03-31\"):\n",
        "    \"\"\"\n",
        "    Fetch daily port-level data from Daily_Ports_Data\n",
        "\n",
        "    Parameters:\n",
        "    - port_ids: List of port IDs (e.g., ['port1325', 'port1198']) or None for all ports\n",
        "    - countries: List of country names or ISO3 codes to filter\n",
        "    - start_date: YYYY-MM-DD\n",
        "    - end_date: YYYY-MM-DD\n",
        "    \"\"\"\n",
        "\n",
        "    base_url = \"https://services9.arcgis.com/weJ1QsnbMYJlCHdG/arcgis/rest/services/Daily_Ports_Data/FeatureServer/0/query\"\n",
        "\n",
        "    # Build where clause - using year, month, day fields for efficient filtering\n",
        "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "\n",
        "    # Create date range conditions using the separate fields\n",
        "    date_conditions = []\n",
        "\n",
        "    # For simplicity, we'll use the string date field which works with comparisons\n",
        "    date_conditions = [f\"date >= '{start_date}'\", f\"date <= '{end_date}'\"]\n",
        "\n",
        "    # Add port filters if specified\n",
        "    if port_ids:\n",
        "        port_list = \"','\".join(port_ids)\n",
        "        date_conditions.append(f\"portid IN ('{port_list}')\")\n",
        "\n",
        "    # Add country filters if specified\n",
        "    if countries:\n",
        "        country_list = \"','\".join(countries)\n",
        "        date_conditions.append(f\"country IN ('{country_list}') OR ISO3 IN ('{country_list}')\")\n",
        "\n",
        "    where_clause = \" AND \".join(date_conditions)\n",
        "\n",
        "    params = {\n",
        "        \"where\": where_clause,\n",
        "        \"outFields\": \"date,year,month,day,portid,portname,country,ISO3,portcalls_roro,portcalls_container,portcalls_dry_bulk,portcalls_tanker\",\n",
        "        \"returnGeometry\": \"false\",\n",
        "        \"resultRecordCount\": \"5000\",\n",
        "        \"maxRecordCountFactor\": \"5\",\n",
        "        \"orderByFields\": \"portid ASC, date ASC\",\n",
        "        \"f\": \"json\"\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸ“¡ Fetching daily port data...\")\n",
        "    print(f\"   Period: {start_date} to {end_date}\")\n",
        "    if port_ids:\n",
        "        print(f\"   Ports: {port_ids}\")\n",
        "    if countries:\n",
        "        print(f\"   Countries: {countries}\")\n",
        "\n",
        "    all_features = []\n",
        "    offset = 0\n",
        "\n",
        "    while True:\n",
        "        params[\"resultOffset\"] = str(offset)\n",
        "\n",
        "        response = requests.get(base_url, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"âŒ Error: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if 'error' in data:\n",
        "            print(f\"âŒ API Error: {data['error'].get('message')}\")\n",
        "            break\n",
        "\n",
        "        features = data.get('features', [])\n",
        "        if not features:\n",
        "            break\n",
        "\n",
        "        all_features.extend(features)\n",
        "        print(f\"   Retrieved {len(all_features)} records so far...\")\n",
        "\n",
        "        if len(features) < 5000:  # Last page\n",
        "            break\n",
        "\n",
        "        offset += 5000\n",
        "\n",
        "    if not all_features:\n",
        "        print(\"âŒ No data returned\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    records = []\n",
        "    for f in all_features:\n",
        "        attrs = f['attributes']\n",
        "        # Convert date string to datetime\n",
        "        if 'date' in attrs:\n",
        "            attrs['date'] = pd.to_datetime(attrs['date'])\n",
        "        records.append(attrs)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(f\"\\nâœ… Retrieved {len(df)} total records\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 1: Get all ports to find UK and French port IDs\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 1: Finding UK and French Port IDs\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ports_url = \"https://services9.arcgis.com/weJ1QsnbMYJlCHdG/arcgis/rest/services/PortWatch_ports_database/FeatureServer/0/query\"\n",
        "params = {\n",
        "    \"where\": \"country IN ('United Kingdom','France','Netherlands','Belgium')\",\n",
        "    \"outFields\": \"portid,portname,country,ISO3,lat,lon\",\n",
        "    \"returnGeometry\": \"false\",\n",
        "    \"f\": \"json\"\n",
        "}\n",
        "\n",
        "response = requests.get(ports_url, params=params)\n",
        "ports_data = response.json()\n",
        "\n",
        "europe_ports = []\n",
        "for feat in ports_data.get('features', []):\n",
        "    attrs = feat['attributes']\n",
        "    europe_ports.append({\n",
        "        'portid': attrs.get('portid'),\n",
        "        'portname': attrs.get('portname'),\n",
        "        'country': attrs.get('country'),\n",
        "        'ISO3': attrs.get('ISO3')\n",
        "    })\n",
        "\n",
        "ports_df = pd.DataFrame(europe_ports)\n",
        "print(f\"\\nFound {len(ports_df)} ports in UK, France, Netherlands, Belgium\")\n",
        "print(\"\\nKey ports:\")\n",
        "key_port_names = ['Dover', 'Calais', 'Rotterdam', 'Zeebrugge', 'Antwerp', 'Felixstowe', 'Southampton']\n",
        "key_ports = ports_df[ports_df['portname'].str.contains('|'.join(key_port_names), case=False, na=False)]\n",
        "print(key_ports.to_string())\n",
        "\n",
        "# Save port IDs for later\n",
        "key_ports.to_csv('key_europe_ports.csv', index=False)\n",
        "port_ids_list = key_ports['portid'].tolist()\n",
        "\n",
        "# Step 2: Fetch FY24/25 daily data for these ports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: Fetching FY24/25 Daily Port Data\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "daily_data = fetch_port_daily_data(\n",
        "    port_ids=port_ids_list,\n",
        "    start_date=\"2024-04-01\",\n",
        "    end_date=\"2026-03-31\"\n",
        ")\n",
        "\n",
        "if not daily_data.empty:\n",
        "    # Save master file\n",
        "    daily_data.to_csv('fy24_25_port_daily_data.csv', index=False)\n",
        "    print(\"\\nðŸ’¾ Saved to 'fy24_25_port_daily_data.csv'\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ðŸ“Š FY24/25 DATA SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    summary = daily_data.groupby(['portname', 'country']).agg({\n",
        "        'portcalls_roro': ['sum', 'mean', 'count'],\n",
        "        'date': ['min', 'max']\n",
        "    }).round(2)\n",
        "\n",
        "    print(summary)\n",
        "\n",
        "    # Daily time series for key ports\n",
        "    for port in daily_data['portname'].unique():\n",
        "        port_df = daily_data[daily_data['portname'] == port].sort_values('date')\n",
        "        print(f\"\\nðŸ“ˆ {port} daily RoRo calls:\")\n",
        "        print(port_df[['date', 'portcalls_roro']].head(10))\n",
        "        print(f\"... and {len(port_df)-10} more days\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "Collecting numpy>=1.26.0 (from pandas)\n",
            "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pandas\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [pandas]2m1/2\u001b[0m [pandas]\n",
            "\u001b[1A\u001b[2KSuccessfully installed numpy-2.4.2 pandas-3.0.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ7lsxcGwWSs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er7HKsyHwBIp",
        "outputId": "bc5697f1-3423-4e64-88a3-d9cf348826d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data for 6 ports: Southampton, Zeebrugge, Calais, Dover, Felixstowe, Antwerp\n",
            "\n",
            "============================================================\n",
            "PORT BASELINE STATISTICS\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Southampton (United Kingdom):\n",
            "   Daily RoRo calls: mean=1.80, std=1.20\n",
            "   Total calls (677 days): 1229\n",
            "   Zero days: 95 (13.9%)\n",
            "   Weekly pattern:\n",
            "     Mon: 1.83\n",
            "     Tue: 1.64\n",
            "     Wed: 1.82\n",
            "     Thu: 1.87\n",
            "     Fri: 1.58\n",
            "     Sat: 1.84\n",
            "     Sun: 2.01\n",
            "\n",
            "ðŸ“Š Zeebrugge (Belgium):\n",
            "   Daily RoRo calls: mean=8.46, std=2.68\n",
            "   Total calls (677 days): 5787\n",
            "   Zero days: 1 (0.1%)\n",
            "   Weekly pattern:\n",
            "     Mon: 6.98\n",
            "     Tue: 9.87\n",
            "     Wed: 7.62\n",
            "     Thu: 9.71\n",
            "     Fri: 8.87\n",
            "     Sat: 8.48\n",
            "     Sun: 7.68\n",
            "\n",
            "ðŸ“Š Calais (France):\n",
            "   Daily RoRo calls: mean=0.10, std=0.30\n",
            "   Total calls (677 days): 67\n",
            "   Zero days: 617 (90.2%)\n",
            "   Weekly pattern:\n",
            "     Mon: 0.07\n",
            "     Tue: 0.13\n",
            "     Wed: 0.02\n",
            "     Thu: 0.17\n",
            "     Fri: 0.07\n",
            "     Sat: 0.10\n",
            "     Sun: 0.11\n",
            "\n",
            "ðŸ“Š Dover (United Kingdom):\n",
            "   Daily RoRo calls: mean=0.00, std=0.04\n",
            "   Total calls (677 days): 1\n",
            "   Zero days: 683 (99.9%)\n",
            "   Weekly pattern:\n",
            "     Mon: 0.01\n",
            "     Tue: 0.00\n",
            "     Wed: 0.00\n",
            "     Thu: 0.00\n",
            "     Fri: 0.00\n",
            "     Sat: 0.00\n",
            "     Sun: 0.00\n",
            "\n",
            "ðŸ“Š Felixstowe (United Kingdom):\n",
            "   Daily RoRo calls: mean=0.68, std=0.93\n",
            "   Total calls (677 days): 467\n",
            "   Zero days: 416 (60.8%)\n",
            "   Weekly pattern:\n",
            "     Mon: 1.72\n",
            "     Tue: 2.16\n",
            "     Wed: 0.13\n",
            "     Thu: 0.04\n",
            "     Fri: 0.06\n",
            "     Sat: 0.44\n",
            "     Sun: 0.21\n",
            "\n",
            "ðŸ“Š Antwerp (Belgium):\n",
            "   Daily RoRo calls: mean=2.46, std=1.45\n",
            "   Total calls (677 days): 1682\n",
            "   Zero days: 46 (6.7%)\n",
            "   Weekly pattern:\n",
            "     Mon: 2.15\n",
            "     Tue: 2.59\n",
            "     Wed: 2.33\n",
            "     Thu: 2.79\n",
            "     Fri: 2.86\n",
            "     Sat: 2.39\n",
            "     Sun: 2.10\n",
            "\n",
            "============================================================\n",
            "RUNNING SCENARIOS FOR Zeebrugge\n",
            "Disruption start date: 2025-07-15 00:00:00\n",
            "============================================================\n",
            "\n",
            "â–¶ Scenario: Minor (10% reduction, 3 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Zeebrugge\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 9.02 calls/day (Ïƒ=2.83)\n",
            "Normal capacity (Î¼): 18.00 calls/day\n",
            "Disrupted capacity (10% reduction): 16.20 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 0 vessels\n",
            "   Peak wait time: 0.0 hours\n",
            "   Total arrivals during disruption: 29\n",
            "   Total processing capacity during disruption: 146\n",
            "\n",
            "â–¶ Scenario: Moderate (25% reduction, 7 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Zeebrugge\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 9.02 calls/day (Ïƒ=2.83)\n",
            "Normal capacity (Î¼): 18.00 calls/day\n",
            "Disrupted capacity (25% reduction): 13.50 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 0 vessels\n",
            "   Peak wait time: 0.9 hours\n",
            "   Total arrivals during disruption: 62\n",
            "   Total processing capacity during disruption: 662\n",
            "\n",
            "â–¶ Scenario: Major (50% reduction, 14 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Zeebrugge\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 9.02 calls/day (Ïƒ=2.83)\n",
            "Normal capacity (Î¼): 18.00 calls/day\n",
            "Disrupted capacity (50% reduction): 9.00 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 7 vessels\n",
            "   Peak wait time: 18.7 hours\n",
            "   Total arrivals during disruption: 124\n",
            "   Total processing capacity during disruption: 1764\n",
            "\n",
            "â–¶ Scenario: Severe (75% reduction, 5 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Zeebrugge\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 9.02 calls/day (Ïƒ=2.83)\n",
            "Normal capacity (Î¼): 18.00 calls/day\n",
            "Disrupted capacity (75% reduction): 4.50 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 28 vessels\n",
            "   Peak wait time: 152.0 hours\n",
            "   Total arrivals during disruption: 51\n",
            "   Total processing capacity during disruption: 112\n",
            "\n",
            "============================================================\n",
            "RUNNING SCENARIOS FOR Southampton\n",
            "Disruption start date: 2025-07-15 00:00:00\n",
            "============================================================\n",
            "\n",
            "â–¶ Scenario: Minor (10% reduction, 3 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Southampton\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.00 calls/day (Ïƒ=1.20)\n",
            "Normal capacity (Î¼): 6.00 calls/day\n",
            "Disrupted capacity (10% reduction): 5.40 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 0 vessels\n",
            "   Peak wait time: 0.0 hours\n",
            "   Total arrivals during disruption: 4\n",
            "   Total processing capacity during disruption: 49\n",
            "\n",
            "â–¶ Scenario: Moderate (25% reduction, 7 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Southampton\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.00 calls/day (Ïƒ=1.20)\n",
            "Normal capacity (Î¼): 6.00 calls/day\n",
            "Disrupted capacity (25% reduction): 4.50 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 0 vessels\n",
            "   Peak wait time: 0.0 hours\n",
            "   Total arrivals during disruption: 10\n",
            "   Total processing capacity during disruption: 220\n",
            "\n",
            "â–¶ Scenario: Major (50% reduction, 14 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Southampton\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.00 calls/day (Ïƒ=1.20)\n",
            "Normal capacity (Î¼): 6.00 calls/day\n",
            "Disrupted capacity (50% reduction): 3.00 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 1 vessels\n",
            "   Peak wait time: 8.0 hours\n",
            "   Total arrivals during disruption: 25\n",
            "   Total processing capacity during disruption: 588\n",
            "\n",
            "â–¶ Scenario: Severe (75% reduction, 5 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Southampton\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.00 calls/day (Ïƒ=1.20)\n",
            "Normal capacity (Î¼): 6.00 calls/day\n",
            "Disrupted capacity (75% reduction): 1.50 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 1 vessels\n",
            "   Peak wait time: 16.0 hours\n",
            "   Total arrivals during disruption: 5\n",
            "   Total processing capacity during disruption: 38\n",
            "\n",
            "âœ… All data saved to CSV files\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv('fy24_25_port_daily_data.csv')\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "class PortDisruptionModel:\n",
        "    \"\"\"Queueing model for port disruption scenarios using actual port data\"\"\"\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.ports = df['portname'].unique()\n",
        "        print(f\"Loaded data for {len(self.ports)} ports: {', '.join(self.ports)}\")\n",
        "\n",
        "    def analyze_port(self, port_name):\n",
        "        \"\"\"Calculate baseline statistics for a specific port\"\"\"\n",
        "        port_df = self.df[self.df['portname'] == port_name].copy()\n",
        "        port_df = port_df.sort_values('date')\n",
        "\n",
        "        # Basic statistics\n",
        "        stats = {\n",
        "            'port': port_name,\n",
        "            'country': port_df['country'].iloc[0],\n",
        "            'days': len(port_df),\n",
        "            'total_calls': port_df['portcalls_roro'].sum(),\n",
        "            'mean_calls': port_df['portcalls_roro'].mean(),\n",
        "            'std_calls': port_df['portcalls_roro'].std(),\n",
        "            'max_calls': port_df['portcalls_roro'].max(),\n",
        "            'zero_days': (port_df['portcalls_roro'] == 0).sum(),\n",
        "            'pct_zero': (port_df['portcalls_roro'] == 0).mean() * 100\n",
        "        }\n",
        "\n",
        "        # Weekly pattern\n",
        "        port_df['day_of_week'] = port_df['date'].dt.dayofweek\n",
        "        weekly = port_df.groupby('day_of_week')['portcalls_roro'].agg(['mean', 'std'])\n",
        "\n",
        "        return stats, weekly, port_df\n",
        "\n",
        "    def simulate_disruption(self, port_name, disruption_date, duration_days,\n",
        "                           capacity_reduction_pct, use_historical=True):\n",
        "        \"\"\"\n",
        "        Simulate queue buildup during disruption\n",
        "\n",
        "        Parameters:\n",
        "        - port_name: Name of port\n",
        "        - disruption_date: Start date (YYYY-MM-DD)\n",
        "        - duration_days: How long disruption lasts\n",
        "        - capacity_reduction_pct: % reduction in processing capacity\n",
        "        - use_historical: If True, use actual arrival patterns; if False, use average\n",
        "        \"\"\"\n",
        "\n",
        "        # Get port data\n",
        "        port_df = self.df[self.df['portname'] == port_name].copy()\n",
        "        port_df = port_df.sort_values('date')\n",
        "\n",
        "        # Calculate baseline (use 60 days before disruption)\n",
        "        baseline_end = pd.to_datetime(disruption_date) - timedelta(days=1)\n",
        "        baseline_start = baseline_end - timedelta(days=60)\n",
        "        baseline = port_df[\n",
        "            (port_df['date'] >= baseline_start) &\n",
        "            (port_df['date'] <= baseline_end)\n",
        "        ]\n",
        "\n",
        "        if len(baseline) < 30:\n",
        "            print(f\"Warning: Only {len(baseline)} days in baseline, using all available data\")\n",
        "            baseline = port_df[port_df['date'] < pd.to_datetime(disruption_date)]\n",
        "\n",
        "        # Calculate arrival rate (Î») from baseline\n",
        "        lambda_mean = baseline['portcalls_roro'].mean()\n",
        "        lambda_std = baseline['portcalls_roro'].std()\n",
        "\n",
        "        # Get processing rate (Î¼) - assume normal capacity can handle peak + 20%\n",
        "        peak_day = baseline['portcalls_roro'].max()\n",
        "        mu_normal = peak_day * 1.2  # 20% spare capacity\n",
        "\n",
        "        # Disrupted processing rate\n",
        "        mu_disrupted = mu_normal * (1 - capacity_reduction_pct/100)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"DISRUPTION SIMULATION: {port_name}\")\n",
        "        print('='*60)\n",
        "        print(f\"Baseline period: {baseline_start.date()} to {baseline_end.date()}\")\n",
        "        print(f\"Normal arrival rate (Î»): {lambda_mean:.2f} calls/day (Ïƒ={lambda_std:.2f})\")\n",
        "        print(f\"Normal capacity (Î¼): {mu_normal:.2f} calls/day\")\n",
        "        print(f\"Disrupted capacity ({capacity_reduction_pct}% reduction): {mu_disrupted:.2f} calls/day\")\n",
        "\n",
        "        # Run simulation\n",
        "        simulation_days = duration_days + 30  # Include recovery period\n",
        "        sim_dates = [pd.to_datetime(disruption_date) + timedelta(days=i)\n",
        "                    for i in range(simulation_days)]\n",
        "\n",
        "        # Use actual arrivals if available, otherwise use random from baseline distribution\n",
        "        queue = 0\n",
        "        results = []\n",
        "\n",
        "        for i, sim_date in enumerate(sim_dates):\n",
        "            # Get actual arrival if in historical data\n",
        "            hist_row = port_df[port_df['date'] == sim_date]\n",
        "\n",
        "            if use_historical and not hist_row.empty:\n",
        "                arrivals = hist_row['portcalls_roro'].iloc[0]\n",
        "            else:\n",
        "                # Simulate arrivals using baseline distribution\n",
        "                arrivals = max(0, np.random.normal(lambda_mean, lambda_std))\n",
        "\n",
        "            # Determine processing capacity\n",
        "            if i < duration_days:\n",
        "                # During disruption\n",
        "                capacity = mu_disrupted\n",
        "            else:\n",
        "                # Recovery period - capacity returns to normal\n",
        "                capacity = mu_normal\n",
        "\n",
        "            # Queueing logic\n",
        "            if queue > 0:\n",
        "                # Process queue first\n",
        "                processed = min(capacity, queue + arrivals)\n",
        "                queue = queue + arrivals - processed\n",
        "            else:\n",
        "                # No queue, process what we can\n",
        "                if arrivals <= capacity:\n",
        "                    queue = 0\n",
        "                else:\n",
        "                    queue = arrivals - capacity\n",
        "\n",
        "            # Calculate wait time\n",
        "            if queue > 0 and capacity > 0:\n",
        "                wait_time_hours = (queue / capacity) * 24\n",
        "            else:\n",
        "                wait_time_hours = 0\n",
        "\n",
        "            results.append({\n",
        "                'date': sim_date,\n",
        "                'day': i,\n",
        "                'arrivals': arrivals,\n",
        "                'capacity': capacity,\n",
        "                'queue': queue,\n",
        "                'wait_time_hours': wait_time_hours,\n",
        "                'phase': 'disruption' if i < duration_days else 'recovery'\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Summary statistics\n",
        "        peak_queue = results_df['queue'].max()\n",
        "        peak_wait = results_df['wait_time_hours'].max()\n",
        "        total_disrupted = results_df[results_df['phase'] == 'disruption']['arrivals'].sum()\n",
        "        total_processed = (results_df[results_df['phase'] == 'disruption']['capacity'].sum() *\n",
        "                          results_df[results_df['phase'] == 'disruption']['day'].count())\n",
        "\n",
        "        print(f\"\\nðŸ“ˆ SIMULATION RESULTS:\")\n",
        "        print(f\"   Peak queue: {peak_queue:.0f} vessels\")\n",
        "        print(f\"   Peak wait time: {peak_wait:.1f} hours\")\n",
        "        print(f\"   Total arrivals during disruption: {total_disrupted:.0f}\")\n",
        "        print(f\"   Total processing capacity during disruption: {total_processed:.0f}\")\n",
        "\n",
        "        if total_disrupted > total_processed:\n",
        "            backlog = total_disrupted - total_processed\n",
        "            print(f\"   âš  BACKLOG: {backlog:.0f} vessels remain after disruption\")\n",
        "            print(f\"   Recovery time: {results_df[results_df['queue']==0].index[0] - duration_days + 1:.0f} days\")\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def run_scenarios(self, port_name):\n",
        "        \"\"\"Run multiple disruption scenarios for a port\"\"\"\n",
        "\n",
        "        scenarios = [\n",
        "            {'name': 'Minor (10% reduction, 3 days)', 'reduction': 10, 'duration': 3},\n",
        "            {'name': 'Moderate (25% reduction, 7 days)', 'reduction': 25, 'duration': 7},\n",
        "            {'name': 'Major (50% reduction, 14 days)', 'reduction': 50, 'duration': 14},\n",
        "            {'name': 'Severe (75% reduction, 5 days)', 'reduction': 75, 'duration': 5},\n",
        "        ]\n",
        "\n",
        "        # Find a typical week in summer 2025\n",
        "        summer_data = self.df[\n",
        "            (self.df['portname'] == port_name) &\n",
        "            (self.df['date'] >= '2025-06-01') &\n",
        "            (self.df['date'] <= '2025-08-31')\n",
        "        ]\n",
        "\n",
        "        if not summer_data.empty:\n",
        "            # Pick a random Tuesday in summer (typical busy day)\n",
        "            tuesdays = summer_data[summer_data['date'].dt.dayofweek == 1]\n",
        "            if not tuesdays.empty:\n",
        "                disruption_date = tuesdays.iloc[len(tuesdays)//2]['date']\n",
        "            else:\n",
        "                disruption_date = '2025-07-01'\n",
        "        else:\n",
        "            disruption_date = '2025-07-01'\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"RUNNING SCENARIOS FOR {port_name}\")\n",
        "        print(f\"Disruption start date: {disruption_date}\")\n",
        "        print('='*60)\n",
        "\n",
        "        results = {}\n",
        "        for scenario in scenarios:\n",
        "            print(f\"\\nâ–¶ Scenario: {scenario['name']}\")\n",
        "            sim_df = self.simulate_disruption(\n",
        "                port_name,\n",
        "                disruption_date,\n",
        "                scenario['duration'],\n",
        "                scenario['reduction']\n",
        "            )\n",
        "            results[scenario['name']] = sim_df\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize model\n",
        "model = PortDisruptionModel(df)\n",
        "\n",
        "# 1. Analyze each port's baseline\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PORT BASELINE STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for port in model.ports:\n",
        "    stats, weekly, _ = model.analyze_port(port)\n",
        "    print(f\"\\nðŸ“Š {port} ({stats['country']}):\")\n",
        "    print(f\"   Daily RoRo calls: mean={stats['mean_calls']:.2f}, std={stats['std_calls']:.2f}\")\n",
        "    print(f\"   Total calls (677 days): {stats['total_calls']:.0f}\")\n",
        "    print(f\"   Zero days: {stats['zero_days']} ({stats['pct_zero']:.1f}%)\")\n",
        "    print(f\"   Weekly pattern:\")\n",
        "    for dow in range(7):\n",
        "        day_name = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][dow]\n",
        "        mean = weekly.loc[dow, 'mean'] if dow in weekly.index else 0\n",
        "        print(f\"     {day_name}: {mean:.2f}\")\n",
        "\n",
        "# 2. Run scenarios for Zeebrugge (busiest RoRo port)\n",
        "zeebrugge_scenarios = model.run_scenarios('Zeebrugge')\n",
        "\n",
        "# 3. Run scenarios for Southampton\n",
        "southampton_scenarios = model.run_scenarios('Southampton')\n",
        "\n",
        "# 4. Save all simulation results\n",
        "for port in model.ports:\n",
        "    port_df = df[df['portname'] == port].copy()\n",
        "    port_df.to_csv(f'{port.lower()}_daily_data.csv', index=False)\n",
        "\n",
        "print(\"\\nâœ… All data saved to CSV files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ_BaVTYlxai"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvse6EqPlurG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIMcLEPGluuq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b741a280",
        "outputId": "d85d994d-5028-4dcc-ba17-5819b916b85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RUNNING SCENARIOS FOR Antwerp\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "RUNNING SCENARIOS FOR Antwerp\n",
            "Disruption start date: 2025-07-15 00:00:00\n",
            "============================================================\n",
            "\n",
            "â–¶ Scenario: Minor (10% reduction, 3 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Antwerp\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.67 calls/day (Ïƒ=1.48)\n",
            "Normal capacity (Î¼): 8.40 calls/day\n",
            "Disrupted capacity (10% reduction): 7.56 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 0 vessels\n",
            "   Peak wait time: 0.0 hours\n",
            "   Total arrivals during disruption: 6\n",
            "   Total processing capacity during disruption: 68\n",
            "\n",
            "â–¶ Scenario: Moderate (25% reduction, 7 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Antwerp\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.67 calls/day (Ïƒ=1.48)\n",
            "Normal capacity (Î¼): 8.40 calls/day\n",
            "Disrupted capacity (25% reduction): 6.30 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 0 vessels\n",
            "   Peak wait time: 0.0 hours\n",
            "   Total arrivals during disruption: 13\n",
            "   Total processing capacity during disruption: 309\n",
            "\n",
            "â–¶ Scenario: Major (50% reduction, 14 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Antwerp\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.67 calls/day (Ïƒ=1.48)\n",
            "Normal capacity (Î¼): 8.40 calls/day\n",
            "Disrupted capacity (50% reduction): 4.20 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 0 vessels\n",
            "   Peak wait time: 0.0 hours\n",
            "   Total arrivals during disruption: 31\n",
            "   Total processing capacity during disruption: 823\n",
            "\n",
            "â–¶ Scenario: Severe (75% reduction, 5 days)\n",
            "\n",
            "============================================================\n",
            "DISRUPTION SIMULATION: Antwerp\n",
            "============================================================\n",
            "Baseline period: 2025-05-15 to 2025-07-14\n",
            "Normal arrival rate (Î»): 2.67 calls/day (Ïƒ=1.48)\n",
            "Normal capacity (Î¼): 8.40 calls/day\n",
            "Disrupted capacity (75% reduction): 2.10 calls/day\n",
            "\n",
            "ðŸ“ˆ SIMULATION RESULTS:\n",
            "   Peak queue: 1 vessels\n",
            "   Peak wait time: 10.3 hours\n",
            "   Total arrivals during disruption: 10\n",
            "   Total processing capacity during disruption: 52\n",
            "\n",
            "============================================================\n",
            "ANTWERP SIMULATION RESULTS SUMMARY\n",
            "============================================================\n",
            "                        Scenario Peak Queue  Peak Wait Space Needed Backlog\n",
            "   Minor (10% reduction, 3 days)  0 vessels  0.0 hours     0 metres    None\n",
            "Moderate (25% reduction, 7 days)  0 vessels  0.0 hours     0 metres    None\n",
            "  Major (50% reduction, 14 days)  0 vessels  0.0 hours     0 metres    None\n",
            "  Severe (75% reduction, 5 days)  1 vessels 10.3 hours    13 metres    None\n",
            "\n",
            "ðŸ“Š Minor (10% reduction, 3 days) DETAILS:\n",
            "  Disruption duration: 3 days\n",
            "  Daily arrivals during disruption: 2.00\n",
            "  Daily capacity during disruption: 7.56\n",
            "  Queue timeline:\n",
            "    Day 0: queue=0 vessels, wait=0.0h\n",
            "    Day 6: queue=0 vessels, wait=0.0h\n",
            "    Day 12: queue=0 vessels, wait=0.0h\n",
            "    Day 18: queue=0 vessels, wait=0.0h\n",
            "    Day 24: queue=0 vessels, wait=0.0h\n",
            "    Day 30: queue=0 vessels, wait=0.0h\n",
            "\n",
            "ðŸ“Š Moderate (25% reduction, 7 days) DETAILS:\n",
            "  Disruption duration: 7 days\n",
            "  Daily arrivals during disruption: 1.86\n",
            "  Daily capacity during disruption: 6.30\n",
            "  Queue timeline:\n",
            "    Day 0: queue=0 vessels, wait=0.0h\n",
            "    Day 7: queue=0 vessels, wait=0.0h\n",
            "    Day 14: queue=0 vessels, wait=0.0h\n",
            "    Day 21: queue=0 vessels, wait=0.0h\n",
            "    Day 28: queue=0 vessels, wait=0.0h\n",
            "    Day 35: queue=0 vessels, wait=0.0h\n",
            "\n",
            "ðŸ“Š Major (50% reduction, 14 days) DETAILS:\n",
            "  Disruption duration: 14 days\n",
            "  Daily arrivals during disruption: 2.21\n",
            "  Daily capacity during disruption: 4.20\n",
            "  Queue timeline:\n",
            "    Day 0: queue=0 vessels, wait=0.0h\n",
            "    Day 8: queue=0 vessels, wait=0.0h\n",
            "    Day 16: queue=0 vessels, wait=0.0h\n",
            "    Day 24: queue=0 vessels, wait=0.0h\n",
            "    Day 32: queue=0 vessels, wait=0.0h\n",
            "    Day 40: queue=0 vessels, wait=0.0h\n",
            "\n",
            "ðŸ“Š Severe (75% reduction, 5 days) DETAILS:\n",
            "  Disruption duration: 5 days\n",
            "  Daily arrivals during disruption: 2.00\n",
            "  Daily capacity during disruption: 2.10\n",
            "  Queue timeline:\n",
            "    Day 0: queue=0 vessels, wait=0.0h\n",
            "    Day 7: queue=0 vessels, wait=0.0h\n",
            "    Day 14: queue=0 vessels, wait=0.0h\n",
            "    Day 21: queue=0 vessels, wait=0.0h\n",
            "    Day 28: queue=0 vessels, wait=0.0h\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RUNNING SCENARIOS FOR Antwerp\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "antwerp_scenarios = model.run_scenarios('Antwerp')\n",
        "\n",
        "# Extract and display the results in a clean table\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANTWERP SIMULATION RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "scenario_results = []\n",
        "for scenario_name, sim_df in antwerp_scenarios.items():\n",
        "    # Extract key metrics\n",
        "    peak_queue = sim_df['queue'].max()\n",
        "    peak_wait = sim_df['wait_time_hours'].max()\n",
        "    total_arrivals = sim_df[sim_df['phase'] == 'disruption']['arrivals'].sum()\n",
        "    total_capacity_during_disruption = sim_df[sim_df['phase'] == 'disruption']['capacity'].sum()\n",
        "    disruption_days = len(sim_df[sim_df['phase'] == 'disruption'])\n",
        "\n",
        "    # Calculate if backlog occurred\n",
        "    # The total processing capacity should be multiplied by the number of days\n",
        "    backlog = max(0, total_arrivals - (total_capacity_during_disruption))\n",
        "\n",
        "    scenario_results.append({\n",
        "        'Scenario': scenario_name,\n",
        "        'Peak Queue': f\"{peak_queue:.0f} vessels\",\n",
        "        'Peak Wait': f\"{peak_wait:.1f} hours\",\n",
        "        'Space Needed': f\"{peak_queue * 15:.0f} metres\", # Assuming 15 meters per vessel\n",
        "        'Backlog': f\"{backlog:.0f} vessels\" if backlog > 0 else \"None\"\n",
        "    })\n",
        "\n",
        "# Display as table\n",
        "results_df = pd.DataFrame(scenario_results)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Detailed breakdown for each scenario\n",
        "for scenario_name, sim_df in antwerp_scenarios.items():\n",
        "    print(f\"\\nðŸ“Š {scenario_name} DETAILS:\")\n",
        "    disruption_days = len(sim_df[sim_df['phase'] == 'disruption'])\n",
        "    print(f\"  Disruption duration: {disruption_days} days\")\n",
        "    print(f\"  Daily arrivals during disruption: {sim_df[sim_df['phase'] == 'disruption']['arrivals'].mean():.2f}\")\n",
        "    print(f\"  Daily capacity during disruption: {sim_df[sim_df['phase'] == 'disruption']['capacity'].mean():.2f}\")\n",
        "    print(f\"  Queue timeline:\")\n",
        "\n",
        "    # Show queue on key days (e.g., every 5 days or at specific points)\n",
        "    # Adjusting for potentially shorter dataframes to avoid errors\n",
        "    step = max(1, len(sim_df)//5)\n",
        "    for day_idx in range(0, len(sim_df), step):\n",
        "        row = sim_df.iloc[day_idx]\n",
        "        print(f\"    Day {row['day']:.0f}: queue={row['queue']:.0f} vessels, wait={row['wait_time_hours']:.1f}h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEv0e9-Plu1A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwKCzUS0lu33"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from scipy.special import expit  # sigmoid function\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class FixedCascadingFailureAnalyzer:\n",
        "    \"\"\"\n",
        "    Fixed version with proper cascade logic and probabilistic propagation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.network_graph = nx.DiGraph()\n",
        "        self.ports_data = {}\n",
        "        self.default_shock = {\n",
        "            'peak_queue': 30,\n",
        "            'peak_wait': 36,\n",
        "            'days_disrupted': 5\n",
        "        }\n",
        "        \n",
        "    def add_port_data(self, port_name, scenario_dict):\n",
        "        \"\"\"\n",
        "        Add simulation data for multiple scenarios of a port\n",
        "        \"\"\"\n",
        "        self.ports_data[port_name] = scenario_dict\n",
        "        \n",
        "    def build_port_network(self, trade_routes=None):\n",
        "        \"\"\"\n",
        "        Build network with proper weight transformations\n",
        "        \"\"\"\n",
        "        # Define routes with proper dependency strengths\n",
        "        default_routes = [\n",
        "            # Format: (origin, destination, volume, strength)\n",
        "            # Benelux hub\n",
        "            ('Rotterdam', 'Antwerp', 8.5, 0.95),\n",
        "            ('Antwerp', 'Rotterdam', 8.5, 0.95),\n",
        "            ('Rotterdam', 'Zeebrugge', 4.2, 0.85),\n",
        "            ('Zeebrugge', 'Rotterdam', 4.2, 0.85),\n",
        "            ('Rotterdam', 'Amsterdam', 3.8, 0.80),\n",
        "            ('Antwerp', 'Zeebrugge', 3.5, 0.90),\n",
        "            \n",
        "            # UK connections\n",
        "            ('Rotterdam', 'Felixstowe', 5.5, 0.95),\n",
        "            ('Rotterdam', 'Southampton', 4.8, 0.92),\n",
        "            ('Antwerp', 'Southampton', 3.2, 0.85),\n",
        "            ('Zeebrugge', 'Dover', 3.8, 0.98),  # Critical channel\n",
        "            ('Calais', 'Dover', 4.2, 0.99),  # Most critical\n",
        "            ('Dover', 'Calais', 4.2, 0.99),  # Bidirectional\n",
        "            ('Le Havre', 'Southampton', 2.8, 0.75),\n",
        "            \n",
        "            # German ports\n",
        "            ('Rotterdam', 'Hamburg', 3.5, 0.80),\n",
        "            ('Hamburg', 'Bremen', 4.0, 0.88),\n",
        "            \n",
        "            # French ports\n",
        "            ('Le Havre', 'Rotterdam', 3.0, 0.78),\n",
        "            ('Le Havre', 'Antwerp', 2.5, 0.70),\n",
        "        ]\n",
        "        \n",
        "        routes = trade_routes if trade_routes else default_routes\n",
        "        \n",
        "        for origin, dest, volume, strength in routes:\n",
        "            self.network_graph.add_edge(origin, dest, \n",
        "                                       volume=volume, \n",
        "                                       strength=strength,\n",
        "                                       # For shortest path: lower weight = more vulnerable\n",
        "                                       # So we use inverse strength\n",
        "                                       weight=1.0/max(strength, 0.01))\n",
        "        \n",
        "        return self.network_graph\n",
        "    \n",
        "    def _propagation_probability(self, source_port, target_port, \n",
        "                                 queue, wait, cascade_strength,\n",
        "                                 q_thresh=20, w_thresh=24):\n",
        "        \"\"\"\n",
        "        Calculate probability of failure propagation using sigmoid function\n",
        "        \n",
        "        More realistic: probability increases smoothly as thresholds are exceeded\n",
        "        \"\"\"\n",
        "        # Normalize excess over thresholds\n",
        "        q_excess = max(0, queue - q_thresh) / q_thresh\n",
        "        w_excess = max(0, wait - w_thresh) / w_thresh\n",
        "        \n",
        "        # Combined stress factor\n",
        "        stress = (q_excess * 0.6 + w_excess * 0.4)  # Queue weighted more\n",
        "        \n",
        "        # Sigmoid parameters\n",
        "        k = 2.0  # Steepness\n",
        "        x0 = 0.5  # Midpoint\n",
        "        \n",
        "        # Base probability from stress\n",
        "        base_prob = expit(k * (stress - x0))\n",
        "        \n",
        "        # Modulate by cascade strength\n",
        "        prob = base_prob * cascade_strength\n",
        "        \n",
        "        return min(1.0, prob)\n",
        "    \n",
        "    def _get_port_impact(self, port_name, reduction_pct, scenario_key=None):\n",
        "        \"\"\"\n",
        "        Get impact metrics for a port, with fallback to default\n",
        "        \"\"\"\n",
        "        if port_name in self.ports_data:\n",
        "            # If specific scenario requested\n",
        "            if scenario_key and scenario_key in self.ports_data[port_name]:\n",
        "                data = self.ports_data[port_name][scenario_key]\n",
        "            else:\n",
        "                # Get the most severe scenario available\n",
        "                scenarios = self.ports_data[port_name]\n",
        "                if scenarios:\n",
        "                    # Find scenario with highest reduction\n",
        "                    def extract_pct(name):\n",
        "                        try:\n",
        "                            return int(name.split('(')[1].split('%')[0])\n",
        "                        except:\n",
        "                            return 0\n",
        "                    \n",
        "                    worst_scenario = max(scenarios.items(), \n",
        "                                        key=lambda x: extract_pct(x[0]))\n",
        "                    data = worst_scenario[1]\n",
        "                else:\n",
        "                    return self.default_shock\n",
        "                    \n",
        "            return {\n",
        "                'peak_queue': data['queue'].max(),\n",
        "                'peak_wait': data['wait_time_hours'].max(),\n",
        "                'days_disrupted': (data['queue'] > 0).sum()\n",
        "            }\n",
        "        else:\n",
        "            # Scale default shock by reduction percentage\n",
        "            scale_factor = reduction_pct / 50.0  # 50% reduction = 1x\n",
        "            return {\n",
        "                'peak_queue': self.default_shock['peak_queue'] * scale_factor,\n",
        "                'peak_wait': self.default_shock['peak_wait'] * scale_factor,\n",
        "                'days_disrupted': self.default_shock['days_disrupted'] * scale_factor\n",
        "            }\n",
        "    \n",
        "    def simulate_cascading_failure(self, initial_failure_port, reduction_pct, \n",
        "                                  scenario_key=None,\n",
        "                                  queue_threshold=20, wait_threshold=24,\n",
        "                                  min_propagation_prob=0.1,\n",
        "                                  allow_missing_data=True,\n",
        "                                  max_generations=10):\n",
        "        \"\"\"\n",
        "        Fixed cascade simulation with proper generation-by-generation propagation\n",
        "        \n",
        "        Parameters:\n",
        "        - initial_failure_port: Port where failure starts\n",
        "        - reduction_pct: Capacity reduction percentage\n",
        "        - scenario_key: Specific scenario to use (e.g., 'Severe (75% reduction, 5 days)')\n",
        "        - queue_threshold/wait_threshold: Thresholds for significant impact\n",
        "        - min_propagation_prob: Minimum probability to consider for propagation\n",
        "        - allow_missing_data: If True, use default shocks for ports without data\n",
        "        - max_generations: Prevent infinite loops\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ðŸŒŠ CASCADING FAILURE SIMULATION (FIXED)\")\n",
        "        print(f\"Initial Failure: {initial_failure_port} ({reduction_pct}% capacity reduction)\")\n",
        "        print(f\"Scenario: {scenario_key if scenario_key else 'Default'}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Track failure propagation\n",
        "        failed_ports = {initial_failure_port: 0}  # port -> generation\n",
        "        cascade_timeline = []\n",
        "        propagation_events = []\n",
        "        \n",
        "        # Record initial failure\n",
        "        initial_impact = self._get_port_impact(initial_failure_port, reduction_pct, scenario_key)\n",
        "        cascade_timeline.append({\n",
        "            'port': initial_failure_port,\n",
        "            'generation': 0,\n",
        "            'peak_queue': initial_impact['peak_queue'],\n",
        "            'peak_wait': initial_impact['peak_wait'],\n",
        "            'days_disrupted': initial_impact['days_disrupted']\n",
        "        })\n",
        "        \n",
        "        print(f\"\\nðŸ“ Generation 0: {initial_failure_port} failed\")\n",
        "        print(f\"   Queue: {initial_impact['peak_queue']:.1f}, Wait: {initial_impact['peak_wait']:.1f}h\")\n",
        "        \n",
        "        # Track frontier (ports that failed in current generation)\n",
        "        current_frontier = [initial_failure_port]\n",
        "        generation = 0\n",
        "        \n",
        "        # Propagate generation by generation\n",
        "        while current_frontier and generation < max_generations:\n",
        "            next_frontier = []\n",
        "            generation += 1\n",
        "            \n",
        "            print(f\"\\nðŸ”„ Generation {generation}:\")\n",
        "            \n",
        "            for source_port in current_frontier:\n",
        "                # Get source impact (for propagation probability)\n",
        "                source_impact = self._get_port_impact(source_port, reduction_pct, scenario_key)\n",
        "                \n",
        "                # Check all downstream ports\n",
        "                downstream_ports = list(self.network_graph.successors(source_port))\n",
        "                \n",
        "                for target_port in downstream_ports:\n",
        "                    # Skip if already failed\n",
        "                    if target_port in failed_ports:\n",
        "                        continue\n",
        "                    \n",
        "                    # Get edge properties\n",
        "                    edge_data = self.network_graph.get_edge_data(source_port, target_port)\n",
        "                    cascade_strength = edge_data['strength']\n",
        "                    \n",
        "                    # Calculate propagation probability\n",
        "                    prob = self._propagation_probability(\n",
        "                        source_port, target_port,\n",
        "                        source_impact['peak_queue'], source_impact['peak_wait'],\n",
        "                        cascade_strength,\n",
        "                        q_thresh=queue_threshold, w_thresh=wait_threshold\n",
        "                    )\n",
        "                    \n",
        "                    # Handle missing data case\n",
        "                    if target_port not in self.ports_data and allow_missing_data:\n",
        "                        # Increase probability for critical links even without data\n",
        "                        if cascade_strength > 0.8:\n",
        "                            prob = max(prob, 0.5)\n",
        "                    \n",
        "                    # Determine if failure propagates (probabilistic)\n",
        "                    if prob >= min_propagation_prob:\n",
        "                        # Get impact on target port\n",
        "                        target_impact = self._get_port_impact(target_port, reduction_pct, scenario_key)\n",
        "                        \n",
        "                        # Scale impact by cascade strength and generation\n",
        "                        scale = cascade_strength * (0.8 ** generation)  # Diminishing returns\n",
        "                        scaled_impact = {\n",
        "                            'peak_queue': target_impact['peak_queue'] * scale,\n",
        "                            'peak_wait': target_impact['peak_wait'] * scale,\n",
        "                            'days_disrupted': target_impact['days_disrupted'] * scale\n",
        "                        }\n",
        "                        \n",
        "                        # Record failure\n",
        "                        failed_ports[target_port] = generation\n",
        "                        next_frontier.append(target_port)\n",
        "                        \n",
        "                        cascade_timeline.append({\n",
        "                            'port': target_port,\n",
        "                            'generation': generation,\n",
        "                            'peak_queue': scaled_impact['peak_queue'],\n",
        "                            'peak_wait': scaled_impact['peak_wait'],\n",
        "                            'days_disrupted': scaled_impact['days_disrupted']\n",
        "                        })\n",
        "                        \n",
        "                        propagation_events.append({\n",
        "                            'from': source_port,\n",
        "                            'to': target_port,\n",
        "                            'generation': generation,\n",
        "                            'probability': prob,\n",
        "                            'strength': cascade_strength\n",
        "                        })\n",
        "                        \n",
        "                        print(f\"   â€¢ {source_port} â†’ {target_port} (p={prob:.2f}, strength={cascade_strength:.2f})\")\n",
        "                        print(f\"     Queue: {scaled_impact['peak_queue']:.1f}, Wait: {scaled_impact['peak_wait']:.1f}h\")\n",
        "            \n",
        "            current_frontier = next_frontier\n",
        "        \n",
        "        # Compile results\n",
        "        results = {\n",
        "            'failed_ports': failed_ports,\n",
        "            'propagation_events': pd.DataFrame(propagation_events) if propagation_events else pd.DataFrame(),\n",
        "            'cascade_timeline': pd.DataFrame(cascade_timeline),\n",
        "            'total_ports_affected': len(failed_ports) - 1,\n",
        "            'max_generation': max(failed_ports.values()) if failed_ports else 0\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"âœ… Cascade Complete: {results['total_ports_affected']} additional ports affected\")\n",
        "        print(f\"   Max generation: {results['max_generation']}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def find_vulnerable_paths(self, source, target):\n",
        "        \"\"\"\n",
        "        Find most vulnerable path between ports\n",
        "        (where vulnerability = high dependency)\n",
        "        \n",
        "        Uses transformed weights so shortest path = most vulnerable\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Method 1: Inverse strength (simpler)\n",
        "            def inv_strength(u, v, d):\n",
        "                strength = d.get('strength', 0.1)\n",
        "                return 1.0 / max(strength, 0.01)\n",
        "            \n",
        "            path_inv = nx.shortest_path(self.network_graph, source, target, \n",
        "                                        weight=inv_strength)\n",
        "            \n",
        "            # Method 2: Negative log (better theoretical foundation)\n",
        "            def neglog_strength(u, v, d):\n",
        "                strength = d.get('strength', 0.1)\n",
        "                return -np.log(max(strength, 0.01))\n",
        "            \n",
        "            path_neglog = nx.shortest_path(self.network_graph, source, target, \n",
        "                                          weight=neglog_strength)\n",
        "            \n",
        "            # Calculate vulnerability score for path\n",
        "            path_edges = [(path_inv[i], path_inv[i+1]) for i in range(len(path_inv)-1)]\n",
        "            path_strength = np.prod([self.network_graph[u][v]['strength'] \n",
        "                                    for u, v in path_edges])\n",
        "            vulnerability = 1.0 - path_strength  # Higher = more vulnerable\n",
        "            \n",
        "            return {\n",
        "                'path': path_inv,\n",
        "                'vulnerability_score': vulnerability,\n",
        "                'edges': path_edges\n",
        "            }\n",
        "            \n",
        "        except nx.NetworkXNoPath:\n",
        "            return {'path': None, 'vulnerability_score': 0, 'edges': []}\n",
        "    \n",
        "    def monte_carlo_cascade(self, initial_port, reduction_pct, \n",
        "                           n_simulations=100, **kwargs):\n",
        "        \"\"\"\n",
        "        Run multiple probabilistic simulations to get distribution of outcomes\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for i in range(n_simulations):\n",
        "            # Set random seed for reproducibility\n",
        "            np.random.seed(i)\n",
        "            \n",
        "            result = self.simulate_cascading_failure(\n",
        "                initial_port, reduction_pct, **kwargs\n",
        "            )\n",
        "            \n",
        "            results.append({\n",
        "                'simulation': i,\n",
        "                'ports_affected': result['total_ports_affected'],\n",
        "                'max_generation': result['max_generation']\n",
        "            })\n",
        "        \n",
        "        results_df = pd.DataFrame(results)\n",
        "        \n",
        "        print(f\"\\nðŸ“Š Monte Carlo Results ({n_simulations} simulations):\")\n",
        "        print(f\"   Average ports affected: {results_df['ports_affected'].mean():.1f} Â± {results_df['ports_affected'].std():.1f}\")\n",
        "        print(f\"   Range: {results_df['ports_affected'].min()} - {results_df['ports_affected'].max()}\")\n",
        "        print(f\"   Probability of cascade >5 ports: {(results_df['ports_affected'] > 5).mean()*100:.1f}%\")\n",
        "        \n",
        "        return results_df\n",
        "\n",
        "# Initialize fixed analyzer\n",
        "analyzer = FixedCascadingFailureAnalyzer()\n",
        "\n",
        "# Build network\n",
        "port_network = analyzer.build_port_network()\n",
        "\n",
        "# Add port data (including all key ports)\n",
        "# Assuming you have scenario data for these ports\n",
        "port_data = {\n",
        "    'Zeebrugge': zeebrugge_scenarios,  # Your existing data\n",
        "    'Southampton': southampton_scenarios,\n",
        "    'Antwerp': antwerp_scenarios,\n",
        "    'Rotterdam': rotterdam_scenarios if 'rotterdam_scenarios' in dir() else None,\n",
        "    'Dover': dover_scenarios if 'dover_scenarios' in dir() else None,\n",
        "    'Calais': calais_scenarios if 'calais_scenarios' in dir() else None,\n",
        "}\n",
        "\n",
        "for port, scenarios in port_data.items():\n",
        "    if scenarios:\n",
        "        analyzer.add_port_data(port, scenarios)\n",
        "\n",
        "# Test fixed cascade with different starting points\n",
        "scenarios_to_test = [\n",
        "    ('Rotterdam', 75, 'Severe (75% reduction, 5 days)'),\n",
        "    ('Dover', 75, 'Severe (75% reduction, 5 days)'),\n",
        "    ('Zeebrugge', 75, 'Severe (75% reduction, 5 days)'),\n",
        "]\n",
        "\n",
        "cascade_results = {}\n",
        "for port, reduction, scenario in scenarios_to_test:\n",
        "    results = analyzer.simulate_cascading_failure(\n",
        "        port, \n",
        "        reduction,\n",
        "        scenario_key=scenario,\n",
        "        allow_missing_data=True  # Allow propagation even without data\n",
        "    )\n",
        "    cascade_results[port] = results\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Network with vulnerability heatmap\n",
        "ax1 = axes[0, 0]\n",
        "pos = nx.spring_layout(port_network, k=2, seed=42)\n",
        "\n",
        "# Calculate vulnerability for each node based on cascade results\n",
        "vulnerability_scores = {}\n",
        "for port in port_network.nodes():\n",
        "    # Count how many simulations this port failed in\n",
        "    fail_count = 0\n",
        "    for start, results in cascade_results.items():\n",
        "        if port in results['failed_ports']:\n",
        "            fail_count += 1\n",
        "    vulnerability_scores[port] = fail_count / len(cascade_results)\n",
        "\n",
        "node_colors = [vulnerability_scores.get(node, 0) for node in port_network.nodes()]\n",
        "nx.draw_networkx_nodes(port_network, pos, \n",
        "                      node_color=node_colors,\n",
        "                      node_size=2000,\n",
        "                      cmap='YlOrRd',\n",
        "                      alpha=0.8,\n",
        "                      ax=ax1)\n",
        "nx.draw_networkx_labels(port_network, pos, font_size=8, ax=ax1)\n",
        "nx.draw_networkx_edges(port_network, pos, alpha=0.3, ax=ax1)\n",
        "ax1.set_title('Port Vulnerability Heatmap\\n(Based on Cascade Simulations)')\n",
        "ax1.axis('off')\n",
        "\n",
        "# 2. Cascade size by starting port\n",
        "ax2 = axes[0, 1]\n",
        "ports = list(cascade_results.keys())\n",
        "affected = [r['total_ports_affected'] for r in cascade_results.values()]\n",
        "generations = [r['max_generation'] for r in cascade_results.values()]\n",
        "\n",
        "x = np.arange(len(ports))\n",
        "width = 0.35\n",
        "ax2.bar(x - width/2, affected, width, label='Ports Affected', color='red', alpha=0.7)\n",
        "ax2.bar(x + width/2, generations, width, label='Cascade Generations', color='blue', alpha=0.7)\n",
        "ax2.set_xlabel('Initial Failure Port')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.set_title('Cascade Impact by Starting Point')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(ports)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Propagation timeline\n",
        "ax3 = axes[0, 2]\n",
        "for start_port, results in cascade_results.items():\n",
        "    timeline = results['cascade_timeline']\n",
        "    gen_counts = timeline.groupby('generation').size()\n",
        "    ax3.plot(gen_counts.index, gen_counts.values, \n",
        "            marker='o', label=start_port, linewidth=2)\n",
        "ax3.set_xlabel('Generation')\n",
        "ax3.set_ylabel('New Failures')\n",
        "ax3.set_title('Failure Propagation Speed')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Path vulnerability analysis\n",
        "ax4 = axes[1, 0]\n",
        "# Find most vulnerable paths\n",
        "critical_pairs = [\n",
        "    ('Rotterdam', 'Felixstowe'),\n",
        "    ('Zeebrugge', 'Dover'),\n",
        "    ('Antwerp', 'Southampton'),\n",
        "    ('Calais', 'Dover'),\n",
        "    ('Rotterdam', 'Hamburg')\n",
        "]\n",
        "\n",
        "path_scores = []\n",
        "for source, target in critical_pairs:\n",
        "    path_info = analyzer.find_vulnerable_paths(source, target)\n",
        "    if path_info['path']:\n",
        "        path_scores.append({\n",
        "            'route': f\"{source}â†’{target}\",\n",
        "            'vulnerability': path_info['vulnerability_score'],\n",
        "            'length': len(path_info['path']) - 1\n",
        "        })\n",
        "\n",
        "path_df = pd.DataFrame(path_scores).sort_values('vulnerability', ascending=False)\n",
        "ax4.barh(range(len(path_df)), path_df['vulnerability'], color='coral')\n",
        "ax4.set_yticks(range(len(path_df)))\n",
        "ax4.set_yticklabels(path_df['route'])\n",
        "ax4.set_xlabel('Vulnerability Score')\n",
        "ax4.set_title('Most Vulnerable Trade Routes')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Monte Carlo distribution\n",
        "ax5 = axes[1, 1]\n",
        "# Run Monte Carlo for Rotterdam\n",
        "mc_results = analyzer.monte_carlo_cascade('Rotterdam', 75, n_simulations=50)\n",
        "ax5.hist(mc_results['ports_affected'], bins=10, edgecolor='black', alpha=0.7)\n",
        "ax5.axvline(mc_results['ports_affected'].mean(), color='red', \n",
        "           linestyle='--', label=f\"Mean: {mc_results['ports_affected'].mean():.1f}\")\n",
        "ax5.set_xlabel('Ports Affected')\n",
        "ax5.set_ylabel('Frequency')\n",
        "ax5.set_title('Monte Carlo Distribution\\nRotterdam Failure (50 simulations)')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Probability matrix\n",
        "ax6 = axes[1, 2]\n",
        "ports_list = list(port_network.nodes())[:8]  # Top 8 ports\n",
        "prob_matrix = np.zeros((len(ports_list), len(ports_list)))\n",
        "\n",
        "for i, source in enumerate(ports_list):\n",
        "    for j, target in enumerate(ports_list):\n",
        "        if source != target:\n",
        "            # Count how many simulations caused sourceâ†’target failure\n",
        "            count = 0\n",
        "            if source in cascade_results:\n",
        "                events = cascade_results[source]['propagation_events']\n",
        "                if not events.empty:\n",
        "                    count = len(events[events['to'] == target])\n",
        "            prob_matrix[i, j] = min(count, 1)  # Cap at 1 for visualization\n",
        "\n",
        "im = ax6.imshow(prob_matrix, cmap='YlOrRd', aspect='auto')\n",
        "ax6.set_xticks(range(len(ports_list)))\n",
        "ax6.set_yticks(range(len(ports_list)))\n",
        "ax6.set_xticklabels(ports_list, rotation=45, ha='right')\n",
        "ax6.set_yticklabels(ports_list)\n",
        "ax6.set_title('Failure Propagation Probability')\n",
        "plt.colorbar(im, ax=ax6)\n",
        "\n",
        "plt.suptitle('ðŸ”§ FIXED: Cascading Failure Analysis with Probabilistic Propagation', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print key fixes and improvements\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… KEY FIXES IMPLEMENTED:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "1.  FIXED: Cascade loop now uses frontier-based generation (stable condition)\n",
        "2.  FIXED: Missing data handling - falls back to scaled default shocks\n",
        "3.  FIXED: Scenario reduction percentages are properly passed\n",
        "4.  FIXED: Path vulnerability uses inverse strength (higher strength = more vulnerable)\n",
        "5.  FIXED: Resilience scores normalized to 0-100\n",
        "6.  FIXED: Propagation is probabilistic using sigmoid function\n",
        "7.  ADDED: Monte Carlo simulation for stochastic analysis\n",
        "8.  ADDED: Proper impact scaling by generation and cascade strength\n",
        "9.  ADDED: Visualization of propagation probabilities\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTXX62Gzlu6X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPhNZFAwlu9Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32MfY2izlvAL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from scipy.stats import lognorm, beta, gamma, poisson\n",
        "from scipy.special import expit\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Rectangle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ProbabilisticCascadeModel:\n",
        "    \"\"\"\n",
        "    Full probabilistic cascade model with uncertainty quantification\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, random_seed=42):\n",
        "        self.network = nx.DiGraph()\n",
        "        self.port_data = {}\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "        \n",
        "        # Store simulation results\n",
        "        self.simulation_results = []\n",
        "        self.risk_metrics = {}\n",
        "        \n",
        "    def build_network(self):\n",
        "        \"\"\"\n",
        "        Build port network with probabilistic edge attributes\n",
        "        \"\"\"\n",
        "        # Ports with base characteristics\n",
        "        ports = {\n",
        "            'Rotterdam': {'throughput': 14.5, 'resilience': 0.7, 'recovery_rate': 0.3},\n",
        "            'Antwerp': {'throughput': 12.0, 'resilience': 0.65, 'recovery_rate': 0.28},\n",
        "            'Hamburg': {'throughput': 8.7, 'resilience': 0.75, 'recovery_rate': 0.32},\n",
        "            'Zeebrugge': {'throughput': 2.3, 'resilience': 0.6, 'recovery_rate': 0.25},\n",
        "            'Southampton': {'throughput': 4.5, 'resilience': 0.72, 'recovery_rate': 0.3},\n",
        "            'Felixstowe': {'throughput': 4.0, 'resilience': 0.68, 'recovery_rate': 0.28},\n",
        "            'Dover': {'throughput': 2.5, 'resilience': 0.55, 'recovery_rate': 0.22},\n",
        "            'Calais': {'throughput': 2.2, 'resilience': 0.58, 'recovery_rate': 0.24},\n",
        "            'Le Havre': {'throughput': 3.2, 'resilience': 0.7, 'recovery_rate': 0.29},\n",
        "            'Bremen': {'throughput': 5.5, 'resilience': 0.73, 'recovery_rate': 0.31},\n",
        "            'Amsterdam': {'throughput': 2.8, 'resilience': 0.64, 'recovery_rate': 0.26}\n",
        "        }\n",
        "        \n",
        "        # Add nodes\n",
        "        for port, attrs in ports.items():\n",
        "            self.network.add_node(port, **attrs)\n",
        "        \n",
        "        # Add edges with probabilistic dependencies\n",
        "        # Format: (source, target, mean_strength, std_strength, daily_volume)\n",
        "        edges = [\n",
        "            # Benelux cluster\n",
        "            ('Rotterdam', 'Antwerp', 0.95, 0.03, 8.5),\n",
        "            ('Antwerp', 'Rotterdam', 0.95, 0.03, 8.5),\n",
        "            ('Rotterdam', 'Zeebrugge', 0.85, 0.05, 4.2),\n",
        "            ('Zeebrugge', 'Rotterdam', 0.85, 0.05, 4.2),\n",
        "            ('Rotterdam', 'Amsterdam', 0.80, 0.06, 3.8),\n",
        "            ('Antwerp', 'Zeebrugge', 0.90, 0.04, 3.5),\n",
        "            \n",
        "            # UK connections\n",
        "            ('Rotterdam', 'Felixstowe', 0.95, 0.03, 5.5),\n",
        "            ('Rotterdam', 'Southampton', 0.92, 0.04, 4.8),\n",
        "            ('Antwerp', 'Southampton', 0.85, 0.05, 3.2),\n",
        "            ('Zeebrugge', 'Dover', 0.98, 0.02, 3.8),\n",
        "            ('Calais', 'Dover', 0.99, 0.01, 4.2),\n",
        "            ('Dover', 'Calais', 0.99, 0.01, 4.2),\n",
        "            ('Le Havre', 'Southampton', 0.75, 0.07, 2.8),\n",
        "            \n",
        "            # German ports\n",
        "            ('Rotterdam', 'Hamburg', 0.80, 0.06, 3.5),\n",
        "            ('Hamburg', 'Bremen', 0.88, 0.04, 4.0),\n",
        "            \n",
        "            # French connections\n",
        "            ('Le Havre', 'Rotterdam', 0.78, 0.06, 3.0),\n",
        "            ('Le Havre', 'Antwerp', 0.70, 0.08, 2.5),\n",
        "        ]\n",
        "        \n",
        "        for source, target, mean_strength, std_strength, volume in edges:\n",
        "            # Store distribution parameters instead of fixed values\n",
        "            self.network.add_edge(source, target,\n",
        "                                mean_strength=mean_strength,\n",
        "                                std_strength=std_strength,\n",
        "                                volume=volume,\n",
        "                                # For shortest path calculations\n",
        "                                weight=1.0/mean_strength)\n",
        "        \n",
        "        return self.network\n",
        "    \n",
        "    def sample_edge_strength(self, source, target):\n",
        "        \"\"\"\n",
        "        Sample a random edge strength from its distribution\n",
        "        \"\"\"\n",
        "        edge = self.network[source][target]\n",
        "        mean = edge['mean_strength']\n",
        "        std = edge['std_strength']\n",
        "        \n",
        "        # Beta distribution is good for [0,1] bounded variables\n",
        "        # Convert mean/std to beta parameters\n",
        "        var = std**2\n",
        "        alpha = mean * (mean*(1-mean)/var - 1)\n",
        "        beta_param = (1-mean) * (mean*(1-mean)/var - 1)\n",
        "        \n",
        "        # Sample and clip to valid range\n",
        "        strength = np.random.beta(max(alpha, 0.1), max(beta_param, 0.1))\n",
        "        return np.clip(strength, 0.01, 0.99)\n",
        "    \n",
        "    def sample_port_resilience(self, port):\n",
        "        \"\"\"\n",
        "        Sample random resilience factor for a port\n",
        "        \"\"\"\n",
        "        base_resilience = self.network.nodes[port]['resilience']\n",
        "        \n",
        "        # Add uncertainty (lognormal distribution around base)\n",
        "        uncertainty = np.random.lognormal(mean=0, sigma=0.2)\n",
        "        resilience = base_resilience * uncertainty\n",
        "        \n",
        "        return np.clip(resilience, 0.2, 1.0)\n",
        "    \n",
        "    def failure_probability(self, queue, wait, capacity_reduction, \n",
        "                           resilience, thresholds=(20, 24)):\n",
        "        \"\"\"\n",
        "        Calculate probability of failure using multiple stress factors\n",
        "        \"\"\"\n",
        "        q_thresh, w_thresh = thresholds\n",
        "        \n",
        "        # Normalize metrics\n",
        "        q_stress = max(0, queue - q_thresh) / q_thresh\n",
        "        w_stress = max(0, wait - w_thresh) / w_thresh\n",
        "        c_stress = capacity_reduction / 100.0\n",
        "        \n",
        "        # Combined stress (weighted)\n",
        "        stress = (0.4 * q_stress + 0.3 * w_stress + 0.3 * c_stress)\n",
        "        \n",
        "        # Resilience reduces probability\n",
        "        effective_stress = stress * (1 - resilience)\n",
        "        \n",
        "        # Logistic function with random variation\n",
        "        logit_p = 3 * (effective_stress - 0.5) + np.random.normal(0, 0.2)\n",
        "        p = expit(logit_p)\n",
        "        \n",
        "        return p\n",
        "    \n",
        "    def simulate_cascade(self, initial_port, capacity_reduction, \n",
        "                        n_days=30, stochastic=True):\n",
        "        \"\"\"\n",
        "        Run a single probabilistic cascade simulation\n",
        "        \"\"\"\n",
        "        # Initialize state\n",
        "        failed_ports = {initial_port: {\n",
        "            'generation': 0,\n",
        "            'day': 0,\n",
        "            'queue': 30 * (capacity_reduction/50),\n",
        "            'wait': 36 * (capacity_reduction/50),\n",
        "            'recovered': False\n",
        "        }}\n",
        "        \n",
        "        # Sample random edge strengths for this simulation\n",
        "        edge_strengths = {}\n",
        "        for u, v in self.network.edges():\n",
        "            if stochastic:\n",
        "                edge_strengths[(u, v)] = self.sample_edge_strength(u, v)\n",
        "            else:\n",
        "                edge_strengths[(u, v)] = self.network[u][v]['mean_strength']\n",
        "        \n",
        "        # Sample port resilience\n",
        "        port_resilience = {}\n",
        "        for port in self.network.nodes():\n",
        "            if stochastic:\n",
        "                port_resilience[port] = self.sample_port_resilience(port)\n",
        "            else:\n",
        "                port_resilience[port] = self.network.nodes[port]['resilience']\n",
        "        \n",
        "        # Track cascade\n",
        "        timeline = []\n",
        "        frontier = [initial_port]\n",
        "        day = 0\n",
        "        \n",
        "        while frontier and day < n_days:\n",
        "            next_frontier = []\n",
        "            \n",
        "            for source in frontier:\n",
        "                source_state = failed_ports[source]\n",
        "                \n",
        "                # Check if source has recovered\n",
        "                if source_state.get('recovered', False):\n",
        "                    continue\n",
        "                \n",
        "                # Try to propagate to neighbors\n",
        "                for target in self.network.successors(source):\n",
        "                    if target in failed_ports:\n",
        "                        continue\n",
        "                    \n",
        "                    # Get edge strength\n",
        "                    strength = edge_strengths.get((source, target), 0.5)\n",
        "                    \n",
        "                    # Calculate failure probability\n",
        "                    p_fail = self.failure_probability(\n",
        "                        source_state['queue'],\n",
        "                        source_state['wait'],\n",
        "                        capacity_reduction,\n",
        "                        port_resilience[target]\n",
        "                    ) * strength\n",
        "                    \n",
        "                    # Determine if failure occurs\n",
        "                    if np.random.random() < p_fail:\n",
        "                        # Calculate impact on target\n",
        "                        impact_factor = strength * np.exp(-0.1 * day)\n",
        "                        target_queue = 20 * impact_factor * (capacity_reduction/50)\n",
        "                        target_wait = 24 * impact_factor * (capacity_reduction/50)\n",
        "                        \n",
        "                        # Record failure\n",
        "                        failed_ports[target] = {\n",
        "                            'generation': failed_ports[source]['generation'] + 1,\n",
        "                            'day': day + 1,\n",
        "                            'queue': target_queue,\n",
        "                            'wait': target_wait,\n",
        "                            'recovered': False,\n",
        "                            'caused_by': source,\n",
        "                            'probability': p_fail\n",
        "                        }\n",
        "                        \n",
        "                        next_frontier.append(target)\n",
        "                        \n",
        "                        timeline.append({\n",
        "                            'day': day + 1,\n",
        "                            'source': source,\n",
        "                            'target': target,\n",
        "                            'probability': p_fail,\n",
        "                            'strength': strength,\n",
        "                            'target_queue': target_queue,\n",
        "                            'target_wait': target_wait,\n",
        "                            'generation': failed_ports[source]['generation'] + 1\n",
        "                        })\n",
        "            \n",
        "            # Update source states (some may recover)\n",
        "            for port in frontier:\n",
        "                if np.random.random() < 0.1:  # 10% daily recovery chance\n",
        "                    failed_ports[port]['recovered'] = True\n",
        "            \n",
        "            frontier = next_frontier\n",
        "            day += 1\n",
        "        \n",
        "        return {\n",
        "            'failed_ports': failed_ports,\n",
        "            'timeline': pd.DataFrame(timeline),\n",
        "            'total_failed': len(failed_ports) - 1,\n",
        "            'max_generation': max([v['generation'] for v in failed_ports.values()]),\n",
        "            'cascade_duration': day\n",
        "        }\n",
        "    \n",
        "    def monte_carlo_simulation(self, initial_port, capacity_reduction,\n",
        "                              n_simulations=1000, parallel=True):\n",
        "        \"\"\"\n",
        "        Run Monte Carlo simulations for probabilistic risk assessment\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ðŸŽ² MONTE CARLO SIMULATION\")\n",
        "        print(f\"Initial Port: {initial_port}\")\n",
        "        print(f\"Capacity Reduction: {capacity_reduction}%\")\n",
        "        print(f\"Simulations: {n_simulations}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        for sim in range(n_simulations):\n",
        "            if sim % 100 == 0:\n",
        "                print(f\"  Progress: {sim}/{n_simulations}\")\n",
        "            \n",
        "            result = self.simulate_cascade(initial_port, capacity_reduction, stochastic=True)\n",
        "            \n",
        "            results.append({\n",
        "                'simulation': sim,\n",
        "                'total_failed': result['total_failed'],\n",
        "                'max_generation': result['max_generation'],\n",
        "                'cascade_duration': result['cascade_duration'],\n",
        "                'timeline': result['timeline']\n",
        "            })\n",
        "        \n",
        "        self.simulation_results = results\n",
        "        \n",
        "        # Calculate risk metrics\n",
        "        self.calculate_risk_metrics(initial_port)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def calculate_risk_metrics(self, initial_port):\n",
        "        \"\"\"\n",
        "        Calculate comprehensive risk metrics from simulations\n",
        "        \"\"\"\n",
        "        failed_counts = [r['total_failed'] for r in self.simulation_results]\n",
        "        \n",
        "        self.risk_metrics = {\n",
        "            'initial_port': initial_port,\n",
        "            'mean_failures': np.mean(failed_counts),\n",
        "            'std_failures': np.std(failed_counts),\n",
        "            'p95_failures': np.percentile(failed_counts, 95),\n",
        "            'p99_failures': np.percentile(failed_counts, 99),\n",
        "            'max_failures': np.max(failed_counts),\n",
        "            'min_failures': np.min(failed_counts),\n",
        "            'prob_catastrophic': np.mean([f > 5 for f in failed_counts]),  # >5 ports\n",
        "            'prob_major': np.mean([(f > 3) & (f <= 5) for f in failed_counts]),\n",
        "            'prob_minor': np.mean([f <= 3 for f in failed_counts])\n",
        "        }\n",
        "        \n",
        "        # Fit distribution\n",
        "        params = stats.lognorm.fit(failed_counts)\n",
        "        self.risk_metrics['distribution_params'] = params\n",
        "        \n",
        "        return self.risk_metrics\n",
        "\n",
        "# Initialize model\n",
        "model = ProbabilisticCascadeModel(random_seed=42)\n",
        "network = model.build_network()\n",
        "\n",
        "# Run Monte Carlo simulations for different ports\n",
        "ports_to_analyze = ['Rotterdam', 'Dover', 'Zeebrugge', 'Antwerp']\n",
        "simulation_results = {}\n",
        "\n",
        "for port in ports_to_analyze:\n",
        "    results = model.monte_carlo_simulation(port, 75, n_simulations=500)\n",
        "    simulation_results[port] = results\n",
        "\n",
        "# Visualization\n",
        "fig = plt.figure(figsize={20, 15})\n",
        "\n",
        "# 1. Probability distributions of cascade sizes\n",
        "ax1 = plt.subplot(3, 3, 1)\n",
        "for port in ports_to_analyze:\n",
        "    failures = [r['total_failed'] for r in simulation_results[port]]\n",
        "    sns.kdeplot(failures, label=port, linewidth=2, ax=ax1)\n",
        "ax1.set_xlabel('Number of Ports Affected')\n",
        "ax1.set_ylabel('Probability Density')\n",
        "ax1.set_title('Probability Distributions of Cascade Sizes')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Cumulative distribution functions\n",
        "ax2 = plt.subplot(3, 3, 2)\n",
        "for port in ports_to_analyze:\n",
        "    failures = np.array([r['total_failed'] for r in simulation_results[port]])\n",
        "    sorted_failures = np.sort(failures)\n",
        "    p = 1. * np.arange(len(sorted_failures)) / (len(sorted_failures) - 1)\n",
        "    ax2.plot(sorted_failures, p, linewidth=2, label=port)\n",
        "ax2.set_xlabel('Number of Ports Affected')\n",
        "ax2.set_ylabel('Cumulative Probability')\n",
        "ax2.set_title('Cumulative Distribution Functions')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Risk matrix (probability vs impact)\n",
        "ax3 = plt.subplot(3, 3, 3)\n",
        "risk_data = []\n",
        "for port in ports_to_analyze:\n",
        "    metrics = model.risk_metrics if port == model.risk_metrics.get('initial_port') else None\n",
        "    if metrics:\n",
        "        risk_data.append({\n",
        "            'Port': port,\n",
        "            'Probability': metrics['prob_major'] + metrics['prob_catastrophic'],\n",
        "            'Impact': metrics['p95_failures'],\n",
        "            'Color': 'red' if metrics['prob_catastrophic'] > 0.3 else 'orange'\n",
        "        })\n",
        "\n",
        "risk_df = pd.DataFrame(risk_data)\n",
        "if not risk_df.empty:\n",
        "    scatter = ax3.scatter(risk_df['Probability'], risk_df['Impact'],\n",
        "                         s=200, c=risk_df['Color'], alpha=0.6, edgecolors='black')\n",
        "    for _, row in risk_df.iterrows():\n",
        "        ax3.annotate(row['Port'], (row['Probability'], row['Impact']),\n",
        "                    xytext=(5, 5), textcoords='offset points')\n",
        "ax3.set_xlabel('Probability of Disruption')\n",
        "ax3.set_ylabel('Expected Impact (95th percentile)')\n",
        "ax3.set_title('Risk Matrix: Probability vs Impact')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Heatmap of failure probabilities\n",
        "ax4 = plt.subplot(3, 3, 4)\n",
        "# Calculate failure probability for each port\n",
        "port_failure_prob = {}\n",
        "for port in ports_to_analyze:\n",
        "    for result in simulation_results[port]:\n",
        "        timeline = result['timeline']\n",
        "        if not timeline.empty:\n",
        "            for _, row in timeline.iterrows():\n",
        "                target = row['target']\n",
        "                if target not in port_failure_prob:\n",
        "                    port_failure_prob[target] = []\n",
        "                port_failure_prob[target].append(1)\n",
        "\n",
        "# Aggregate probabilities\n",
        "prob_matrix = np.zeros((len(ports_to_analyze), len(network.nodes())))\n",
        "for i, source in enumerate(ports_to_analyze):\n",
        "    for j, target in enumerate(network.nodes()):\n",
        "        if target in port_failure_prob:\n",
        "            prob_matrix[i, j] = len(port_failure_prob[target]) / (len(simulation_results[source]) * 500)\n",
        "\n",
        "sns.heatmap(prob_matrix, xticklabels=list(network.nodes()), \n",
        "            yticklabels=ports_to_analyze, annot=True, fmt='.2f',\n",
        "            cmap='YlOrRd', cbar_kws={'label': 'Failure Probability'},\n",
        "            ax=ax4)\n",
        "ax4.set_title('Port-to-Port Failure Probabilities')\n",
        "plt.setp(ax4.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# 5. Cascade duration distribution\n",
        "ax5 = plt.subplot(3, 3, 5)\n",
        "for port in ports_to_analyze:\n",
        "    durations = [r['cascade_duration'] for r in simulation_results[port]]\n",
        "    ax5.hist(durations, alpha=0.5, bins=15, label=port, density=True)\n",
        "ax5.set_xlabel('Cascade Duration (days)')\n",
        "ax5.set_ylabel('Probability Density')\n",
        "ax5.set_title('Distribution of Cascade Durations')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Generation depth distribution\n",
        "ax6 = plt.subplot(3, 3, 6)\n",
        "for port in ports_to_analyze:\n",
        "    generations = [r['max_generation'] for r in simulation_results[port]]\n",
        "    counts, bins = np.histogram(generations, bins=range(8))\n",
        "    ax6.plot(bins[:-1], counts/len(simulation_results[port]), \n",
        "            marker='o', label=port, linewidth=2)\n",
        "ax6.set_xlabel('Maximum Cascade Generation')\n",
        "ax6.set_ylabel('Probability')\n",
        "ax6.set_title('Cascade Depth Distribution')\n",
        "ax6.legend()\n",
        "ax6.grid(True, alpha=0.3)\n",
        "\n",
        "# 7. Expected cascade tree\n",
        "ax7 = plt.subplot(3, 3, 7)\n",
        "# Create expected cascade for Rotterdam\n",
        "if 'Rotterdam' in simulation_results:\n",
        "    # Count transitions\n",
        "    transitions = {}\n",
        "    for result in simulation_results['Rotterdam']:\n",
        "        timeline = result['timeline']\n",
        "        for _, row in timeline.iterrows():\n",
        "            key = (row['source'], row['target'])\n",
        "            transitions[key] = transitions.get(key, 0) + 1\n",
        "    \n",
        "    # Normalize\n",
        "    total = len(simulation_results['Rotterdam'])\n",
        "    transition_probs = {k: v/total for k, v in transitions.items()}\n",
        "    \n",
        "    # Draw tree\n",
        "    G_tree = nx.DiGraph()\n",
        "    pos = {}\n",
        "    for (s, t), prob in transition_probs.items():\n",
        "        if prob > 0.1:  # Only show probable transitions\n",
        "            G_tree.add_edge(s, t, weight=prob)\n",
        "    \n",
        "    if len(G_tree.nodes()) > 0:\n",
        "        pos_tree = nx.spring_layout(G_tree, k=2, seed=42)\n",
        "        nx.draw_networkx_nodes(G_tree, pos_tree, node_size=2000, \n",
        "                              node_color='lightblue', ax=ax7)\n",
        "        nx.draw_networkx_labels(G_tree, pos_tree, font_size=8, ax=ax7)\n",
        "        \n",
        "        edges = G_tree.edges()\n",
        "        weights = [G_tree[u][v]['weight'] * 3 for u, v in edges]\n",
        "        nx.draw_networkx_edges(G_tree, pos_tree, width=weights, \n",
        "                              alpha=0.6, edge_color='red', ax=ax7)\n",
        "        \n",
        "ax7.set_title('Expected Cascade Tree (Rotterdam)\\nWidth = Probability')\n",
        "ax7.axis('off')\n",
        "\n",
        "# 8. Risk exceedance curve\n",
        "ax8 = plt.subplot(3, 3, 8)\n",
        "for port in ports_to_analyze:\n",
        "    failures = np.array([r['total_failed'] for r in simulation_results[port]])\n",
        "    sorted_failures = np.sort(failures)[::-1]\n",
        "    exceedance_prob = 1. * np.arange(1, len(sorted_failures) + 1) / len(sorted_failures)\n",
        "    ax8.plot(sorted_failures, exceedance_prob, linewidth=2, label=port)\n",
        "ax8.set_xlabel('Number of Ports Affected (x)')\n",
        "ax8.set_ylabel('P(X > x)')\n",
        "ax8.set_title('Risk Exceedance Curves')\n",
        "ax8.set_yscale('log')\n",
        "ax8.legend()\n",
        "ax8.grid(True, alpha=0.3)\n",
        "\n",
        "# 9. Risk summary table\n",
        "ax9 = plt.subplot(3, 3, 9)\n",
        "ax9.axis('tight')\n",
        "ax9.axis('off')\n",
        "\n",
        "summary_data = []\n",
        "for port in ports_to_analyze:\n",
        "    metrics = model.risk_metrics if port == model.risk_metrics.get('initial_port') else None\n",
        "    if metrics:\n",
        "        summary_data.append([\n",
        "            port,\n",
        "            f\"{metrics['mean_failures']:.1f} Â± {metrics['std_failures']:.1f}\",\n",
        "            f\"{metrics['p95_failures']:.0f}\",\n",
        "            f\"{metrics['prob_minor']*100:.0f}%\",\n",
        "            f\"{metrics['prob_major']*100:.0f}%\",\n",
        "            f\"{metrics['prob_catastrophic']*100:.0f}%\"\n",
        "        ])\n",
        "\n",
        "if summary_data:\n",
        "    table = ax9.table(cellText=summary_data,\n",
        "                      colLabels=['Port', 'Mean Â± Std', '95th %ile', \n",
        "                                'Minor', 'Major', 'Catastrophic'],\n",
        "                      cellLoc='center',\n",
        "                      loc='center',\n",
        "                      colColours=['#4472C4']*6)\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1.2, 1.5)\n",
        "    ax9.set_title('Risk Summary Statistics', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "plt.suptitle('ðŸŽ² Probabilistic Cascade Modeling: Risk Assessment Under Uncertainty', \n",
        "             fontsize=16, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print probabilistic insights\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š PROBABILISTIC RISK ASSESSMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for port in ports_to_analyze:\n",
        "    failures = [r['total_failed'] for r in simulation_results[port]]\n",
        "    \n",
        "    print(f\"\\nðŸ“ {port}:\")\n",
        "    print(f\"   Expected cascade size: {np.mean(failures):.2f} ports (Â±{np.std(failures):.2f})\")\n",
        "    print(f\"   95% confidence interval: [{np.percentile(failures, 2.5):.1f}, {np.percentile(failures, 97.5):.1f}]\")\n",
        "    print(f\"   Probability of cascade >5 ports: {np.mean([f > 5 for f in failures])*100:.1f}%\")\n",
        "    print(f\"   Maximum simulated cascade: {np.max(failures)} ports\")\n",
        "    \n",
        "    # Fit distribution\n",
        "    params = stats.lognorm.fit(failures)\n",
        "    print(f\"   Best fit distribution: LogNormal(shape={params[0]:.2f})\")\n",
        "\n",
        "# Value at Risk (VaR) and Conditional VaR\n",
        "print(\"\\nðŸ“ˆ RISK METRICS (95% confidence):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for port in ports_to_analyze:\n",
        "    failures = np.array([r['total_failed'] for r in simulation_results[port]])\n",
        "    var_95 = np.percentile(failures, 95)\n",
        "    cvar_95 = np.mean(failures[failures > var_95])\n",
        "    \n",
        "    print(f\"\\n{port}:\")\n",
        "    print(f\"   VaR(95%): {var_95:.1f} ports (worst-case with 95% confidence)\")\n",
        "    print(f\"   CVaR(95%): {cvar_95:.1f} ports (expected if exceed VaR)\")\n",
        "\n",
        "# Sensitivity analysis\n",
        "print(\"\\nðŸ” SENSITIVITY ANALYSIS:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Calculate which ports are most sensitive to initial failure\n",
        "sensitivity = {}\n",
        "for port in ports_to_analyze:\n",
        "    failures = [r['total_failed'] for r in simulation_results[port]]\n",
        "    sensitivity[port] = np.mean(failures)\n",
        "\n",
        "sorted_sensitivity = sorted(sensitivity.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nPorts ranked by systemic impact when they fail first:\")\n",
        "for port, impact in sorted_sensitivity:\n",
        "    print(f\"   {port}: {impact:.2f} average ports affected\")\n",
        "\n",
        "# Recommendations based on probabilistic analysis\n",
        "print(\"\\nðŸ›¡ï¸ PROBABILISTIC RISK RECOMMENDATIONS:\")\n",
        "print(\"-\" * 40)\n",
        "print(\"\"\"\n",
        "1.  INVEST IN REDUNDANCY: Focus on ports with high CVaR\n",
        "2.  EARLY WARNING SYSTEMS: Monitor ports with high cascade probability\n",
        "3.  RISK TRANSFER: Consider insurance for catastrophic scenarios (>95th percentile)\n",
        "4.  ADAPTIVE CAPACITY: Build flexibility to handle distribution tail events\n",
        "5.  SCENARIO PLANNING: Use full probability distributions, not just averages\n",
        "6.  REAL-TIME MONITORING: Track conditional probabilities during disruptions\n",
        "7.  PORTFOLIO DIVERSIFICATION: Spread risk across multiple ports\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWiG0YGVlvZS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZcPtu4FwKt1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOCy49BV0+4VLXrf0eHoGL8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPohch4DNb3eyJ5AW8lBEMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlajideFemi/Carbon-Footprint/blob/main/Another_copy_of_Energy_Consumption_Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Energy Consumption Forecasting"
      ],
      "metadata": {
        "id": "8-HqIc-UJejw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><b>Problem Statement</b></p>\n",
        "\n",
        "The objective is to forecast electricity demand over future time periods in order to support operational planning, cost control, and infrastructure resilience.\n",
        "Accurate demand forecasting is critical because electricity systems must balance supply and demand in real time. Over- or under-estimation can result in service disruption, increased costs, or system instability.\n"
      ],
      "metadata": {
        "id": "QLgqddNqJoct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load demand\n",
        "demand = pd.read_csv(\"historic_demand_2009_2024.csv\")\n",
        "\n",
        "# Convert 'settlement_date' to datetime and create 'timestamp'\n",
        "demand[\"settlement_date\"] = pd.to_datetime(demand[\"settlement_date\"])\n",
        "demand[\"timestamp\"] = (\n",
        "    demand[\"settlement_date\"]\n",
        "    + pd.to_timedelta((demand[\"settlement_period\"] - 1) * 30, unit=\"min\")\n",
        ")\n",
        "\n",
        "# Set timestamp as index and sort\n",
        "demand = demand.set_index(\"timestamp\").sort_index()\n",
        "\n",
        "# Drop original date and period columns along with 'Unnamed: 0'\n",
        "columns_to_drop = ['Unnamed: 0', 'settlement_date', 'settlement_period']\n",
        "demand = demand.drop(columns=[col for col in columns_to_drop if col in demand.columns])\n",
        "\n",
        "# Load weather (if applicable, commented out as per original notebook)\n",
        "#weather = pd.read_csv(\"uk_weather.csv\", parse_dates=[\"date\"])"
      ],
      "metadata": {
        "id": "LmCcJV-0KKI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand.head()"
      ],
      "metadata": {
        "id": "EEvTz6_QmWdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand.dtypes"
      ],
      "metadata": {
        "id": "3XIU-x5smpLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand.shape"
      ],
      "metadata": {
        "id": "bkcbFJM4myJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand.describe()"
      ],
      "metadata": {
        "id": "K2lOhQILnWB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "demand[\"settlement_date\"] = pd.to_datetime(demand[\"settlement_date\"])\n",
        "\n",
        "demand[\"timestamp\"] = (\n",
        "    demand[\"settlement_date\"]\n",
        "    + pd.to_timedelta((demand[\"settlement_period\"] - 1) * 30, unit=\"min\")\n",
        ")\n",
        "\n",
        "demand = demand.set_index(\"timestamp\").sort_index()\n"
      ],
      "metadata": {
        "id": "dwyfL2fyHfzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand[\"england_wales_demand\"].plot(figsize=(15,5))\n"
      ],
      "metadata": {
        "id": "8bHI81TqITwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand[\"england_wales_demand\"].describe()\n"
      ],
      "metadata": {
        "id": "3sdlpP2QK8BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand[\"bad_reading\"] = demand[\"england_wales_demand\"] < 10000\n"
      ],
      "metadata": {
        "id": "qQ6m1NMzIre8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand[demand[\"bad_reading\"]][\"england_wales_demand\"].count()\n"
      ],
      "metadata": {
        "id": "kElpp9xGLlEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand.loc[demand[\"bad_reading\"], \"england_wales_demand\"] = None\n",
        "demand[\"england_wales_demand\"] = demand[\"england_wales_demand\"].interpolate(method=\"time\")\n"
      ],
      "metadata": {
        "id": "O4bJmH0iL3YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand.loc[\"2022\"][\"england_wales_demand\"].plot(figsize=(15,4))\n"
      ],
      "metadata": {
        "id": "ffxqmKJ-L_l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand[\"hour\"] = demand.index.hour\n",
        "\n",
        "demand.groupby(\"hour\")[\"england_wales_demand\"].mean().plot()\n"
      ],
      "metadata": {
        "id": "JHgnbH2HMEo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand.groupby(demand.index.dayofweek)[\"england_wales_demand\"].mean().plot()\n"
      ],
      "metadata": {
        "id": "MmUABYInMOoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demand[\"month\"] = demand.index.month\n",
        "\n",
        "demand.boxplot(\n",
        "    column=\"england_wales_demand\",\n",
        "    by=\"month\",\n",
        "    figsize=(12,5)\n",
        ")\n"
      ],
      "metadata": {
        "id": "ML7RIkzPMdse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the data\n",
        "demand = pd.read_csv(\"historic_demand_2009_2024.csv\")\n",
        "\n",
        "\n",
        "demand['timestamp'] = pd.to_datetime(demand['settlement_date']) + \\\n",
        "                     pd.to_timedelta((demand['settlement_period'] - 1) * 30, unit='m')\n",
        "\n",
        "# Set timestamp as index\n",
        "demand.set_index('timestamp', inplace=True)\n",
        "demand.sort_index(inplace=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "demand = demand.drop(columns=['Unnamed: 0', 'settlement_date', 'settlement_period'])\n",
        "\n",
        "# Check data types\n",
        "print(\"Data types:\")\n",
        "print(demand.dtypes)\n",
        "print(\"\\nMissing values:\")\n",
        "print(demand.isnull().sum())"
      ],
      "metadata": {
        "id": "qcB7KMzqMoS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, recreate the timestamp correctly (based on your data structure)\n",
        "# Assuming settlement_period is 1-48 (half-hourly)\n",
        "demand['timestamp'] = pd.to_datetime(demand['settlement_date']) + \\\n",
        "                     pd.to_timedelta((demand['settlement_period'] - 1) * 30, unit='m')\n",
        "\n",
        "# Set timestamp as index\n",
        "demand.set_index('timestamp', inplace=True)\n",
        "demand.sort_index(inplace=True)\n",
        "\n",
        "# Drop unnecessary columns if they exist\n",
        "columns_to_drop = ['Unnamed: 0', 'settlement_date', 'settlement_period']\n",
        "demand = demand.drop(columns=[col for col in columns_to_drop if col in demand.columns])\n",
        "\n",
        "print(f\"Data shape: {demand.shape}\")\n",
        "print(f\"Date range: {demand.index.min()} to {demand.index.max()}\")\n",
        "print(f\"\\nMissing values summary:\")\n",
        "print(demand.isnull().sum().sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "QnTHBj28eQ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's see what columns are actually in your DataFrame\n",
        "print(\"Columns in demand DataFrame:\")\n",
        "print(demand.columns.tolist())\n",
        "print(f\"\\nDataFrame shape: {demand.shape}\")\n",
        "\n",
        "# Let's see the first few rows to understand the structure\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(demand.head())"
      ],
      "metadata": {
        "id": "tXr-qYbIe5Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=== DATA VALIDATION ===\\n\")\n",
        "\n",
        "# Check the date range and frequency\n",
        "print(f\"Data date range: {demand.index.min()} to {demand.index.max()}\")\n",
        "print(f\"Number of periods: {len(demand):,}\")\n",
        "print(f\"Expected periods for 15.5 years (2009-2024): {15.5 * 365.25 * 48:,.0f}\")\n",
        "\n",
        "# Check for missing timestamps\n",
        "full_range = pd.date_range(start=demand.index.min(), end=demand.index.max(), freq='30min')\n",
        "missing_timestamps = full_range.difference(demand.index)\n",
        "print(f\"Missing timestamps: {len(missing_timestamps)}\")\n",
        "\n",
        "# Check data consistency\n",
        "print(f\"\\nData frequency: {pd.infer_freq(demand.index)}\")\n",
        "print(f\"Is index monotonic increasing? {demand.index.is_monotonic_increasing}\")\n",
        "print(f\"Is index unique? {demand.index.is_unique}\")\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = demand.index.duplicated().sum()\n",
        "print(f\"Duplicate timestamps: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(\"Removing duplicates...\")\n",
        "    demand = demand[~demand.index.duplicated(keep='first')]\n",
        "\n",
        "print(\"\\n=== BASIC STATISTICS ===\")\n",
        "print(f\"Shape after cleaning: {demand.shape}\")"
      ],
      "metadata": {
        "id": "wDYp5IVXgL-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PCEna_nhhBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_comprehensive_features(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive features for electricity demand forecasting\n",
        "    \"\"\"\n",
        "    df_feat = df.copy()\n",
        "\n",
        "    # 1. Temporal Features\n",
        "    df_feat['hour'] = df_feat.index.hour\n",
        "    df_feat['minute'] = df_feat.index.minute\n",
        "    df_feat['half_hour_period'] = df_feat['hour'] * 2 + (df_feat['minute'] // 30)\n",
        "    df_feat['day_of_week'] = df_feat.index.dayofweek  # Monday=0, Sunday=6\n",
        "    df_feat['day_of_month'] = df_feat.index.day\n",
        "    df_feat['month'] = df_feat.index.month\n",
        "    df_feat['quarter'] = df_feat.index.quarter\n",
        "    df_feat['year'] = df_feat.index.year\n",
        "    df_feat['day_of_year'] = df_feat.index.dayofyear\n",
        "    df_feat['week_of_year'] = df_feat.index.isocalendar().week.astype(int) # Ensure int type\n",
        "\n",
        "    # 2. Calendar Features\n",
        "    df_feat['is_weekend'] = (df_feat['day_of_week'] >= 5).astype(int)\n",
        "    df_feat['is_weekday'] = (df_feat['day_of_week'] < 5).astype(int)\n",
        "    # Ensure 'is_holiday' is treated as numeric, fill NaNs if any after dropping original columns\n",
        "    df_feat['is_holiday'] = df_feat['is_holiday'].fillna(0).astype(int)\n",
        "    df_feat['is_working_day'] = ((df_feat['day_of_week'] < 5) & (df_feat['is_holiday'] == 0)).astype(int)\n",
        "\n",
        "    # Special periods\n",
        "    df_feat['is_morning_peak'] = ((df_feat['hour'] >= 7) & (df_feat['hour'] <= 10)).astype(int)\n",
        "    df_feat['is_evening_peak'] = ((df_feat['hour'] >= 16) & (df_feat['hour'] <= 20)).astype(int)\n",
        "    df_feat['is_overnight'] = ((df_feat['hour'] >= 0) & (df_feat['hour'] <= 5)).astype(int)\n",
        "\n",
        "    # 3. Calculate Net Demand\n",
        "    df_feat['embedded_total_generation'] = df_feat['embedded_wind_generation'] + df_feat['embedded_solar_generation']\n",
        "    df_feat['net_demand'] = df_feat['england_wales_demand'] - df_feat['embedded_total_generation']\n",
        "\n",
        "    # 4. Capacity Factors\n",
        "    # Avoid division by zero\n",
        "    df_feat['wind_capacity_factor'] = np.where(\n",
        "        df_feat['embedded_wind_capacity'] > 0,\n",
        "        df_feat['embedded_wind_generation'] / df_feat['embedded_wind_capacity'],\n",
        "        0\n",
        "    )\n",
        "    df_feat['solar_capacity_factor'] = np.where(\n",
        "        df_feat['embedded_solar_capacity'] > 0,\n",
        "        df_feat['embedded_solar_generation'] / df_feat['embedded_solar_capacity'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 5. Interconnector Analysis\n",
        "    interconnector_cols = [col for col in df.columns if '_flow' in col]\n",
        "\n",
        "    # Fill missing values in interconnectors (new interconnectors commissioned over time)\n",
        "    for col in interconnector_cols:\n",
        "        # Fill forward, then backward, then 0 - using updated syntax\n",
        "        df_feat[col] = df_feat[col].ffill().bfill().fillna(0)\n",
        "\n",
        "    # Calculate interconnector statistics\n",
        "    df_feat['total_interconnector_flow'] = df_feat[interconnector_cols].sum(axis=1)\n",
        "    df_feat['interconnector_imports'] = df_feat[interconnector_cols].clip(lower=0).sum(axis=1)\n",
        "    df_feat['interconnector_exports'] = df_feat[interconnector_cols].clip(upper=0).abs().sum(axis=1)\n",
        "\n",
        "    # 6. Cyclical Encoding for Time Features\n",
        "    # Hour encoding\n",
        "    df_feat['hour_sin'] = np.sin(2 * np.pi * df_feat['hour'] / 24)\n",
        "    df_feat['hour_cos'] = np.cos(2 * np.pi * df_feat['hour'] / 24)\n",
        "\n",
        "    # Day of week encoding\n",
        "    df_feat['day_of_week_sin'] = np.sin(2 * np.pi * df_feat['day_of_week'] / 7)\n",
        "    df_feat['day_of_week_cos'] = np.cos(2 * np.pi * df_feat['day_of_week'] / 7)\n",
        "\n",
        "    # Month encoding\n",
        "    df_feat['month_sin'] = np.sin(2 * np.pi * df_feat['month'] / 12)\n",
        "    df_feat['month_cos'] = np.cos(2 * np.pi * df_feat['month'] / 12)\n",
        "\n",
        "    # 7. Lag Features (Autoregressive)\n",
        "    # Previous periods\n",
        "    df_feat['demand_lag_1'] = df_feat['england_wales_demand'].shift(1)  # Previous half-hour\n",
        "    df_feat['demand_lag_2'] = df_feat['england_wales_demand'].shift(2)  # Previous hour\n",
        "    df_feat['demand_lag_48'] = df_feat['england_wales_demand'].shift(48)  # Previous day same period\n",
        "    df_feat['demand_lag_336'] = df_feat['england_wales_demand'].shift(336)  # Previous week same period\n",
        "\n",
        "    # Net demand lags\n",
        "    df_feat['net_demand_lag_48'] = df_feat['net_demand'].shift(48)\n",
        "\n",
        "    # 8. Rolling Statistics\n",
        "    # Short-term (daily) patterns\n",
        "    df_feat['demand_rolling_24h_mean'] = df_feat['england_wales_demand'].rolling(window=48, min_periods=1).mean()\n",
        "    df_feat['demand_rolling_24h_std'] = df_feat['england_wales_demand'].rolling(window=48, min_periods=1).std()\n",
        "\n",
        "    # Medium-term (weekly) patterns\n",
        "    df_feat['demand_rolling_7d_mean'] = df_feat['england_wales_demand'].rolling(window=336, min_periods=1).mean()\n",
        "    df_feat['demand_rolling_7d_std'] = df_feat['england_wales_demand'].rolling(window=336, min_periods=1).std()\n",
        "\n",
        "    # 9. Rate of Change\n",
        "    df_feat['demand_change_1h'] = df_feat['england_wales_demand'].diff(2)  # Change over 1 hour\n",
        "    df_feat['demand_change_24h'] = df_feat['england_wales_demand'].diff(48)  # Change over 24 hours\n",
        "\n",
        "    # 10. Penetration Rates\n",
        "    df_feat['wind_penetration'] = df_feat['embedded_wind_generation'] / df_feat['england_wales_demand'].replace(0, np.nan)\n",
        "    df_feat['solar_penetration'] = df_feat['embedded_solar_generation'] / df_feat['england_wales_demand'].replace(0, np.nan)\n",
        "    df_feat['total_renewable_penetration'] = (df_feat['embedded_wind_generation'] + df_feat['embedded_solar_generation']) / df_feat['england_wales_demand'].replace(0, np.nan)\n",
        "\n",
        "    # 11. Season Indicators\n",
        "    seasons = {1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "               6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Autumn', 10: 'Autumn',\n",
        "               11: 'Autumn', 12: 'Winter'}\n",
        "    df_feat['season'] = df_feat['month'].map(seasons)\n",
        "\n",
        "    # Create dummy variables for seasons\n",
        "    season_dummies = pd.get_dummies(df_feat['season'], prefix='season')\n",
        "    df_feat = pd.concat([df_feat, season_dummies], axis=1)\n",
        "    df_feat.drop('season', axis=1, inplace=True)\n",
        "\n",
        "    # 12. Time of Day Categories\n",
        "    time_of_day = []\n",
        "    for h in df_feat['hour']:\n",
        "        if 0 <= h < 6:\n",
        "            time_of_day.append('Night')\n",
        "        elif 6 <= h < 9:\n",
        "            time_of_day.append('Morning Peak')\n",
        "        elif 9 <= h < 16:\n",
        "            time_of_day.append('Day')\n",
        "        elif 16 <= h < 19:\n",
        "            time_of_day.append('Evening Peak')\n",
        "        else:\n",
        "            time_of_day.append('Evening')\n",
        "    df_feat['time_of_day'] = time_of_day\n",
        "\n",
        "    # Create dummy variables for time of day\n",
        "    time_dummies = pd.get_dummies(df_feat['time_of_day'], prefix='time')\n",
        "    df_feat = pd.concat([df_feat, time_dummies], axis=1)\n",
        "    df_feat.drop('time_of_day', axis=1, inplace=True)\n",
        "\n",
        "    # 13. Fill any remaining NaN values\n",
        "    # Forward fill for lag features, then backward fill - using updated syntax\n",
        "    df_feat = df_feat.ffill().bfill()\n",
        "\n",
        "    # Final check for any remaining NaN\n",
        "    if df_feat.isnull().sum().sum() > 0:\n",
        "        print(f\"Warning: {df_feat.isnull().sum().sum()} NaN values remaining\")\n",
        "        df_feat = df_feat.fillna(0)\n",
        "\n",
        "    # Convert boolean columns created by get_dummies to int\n",
        "    for col in df_feat.columns:\n",
        "        if df_feat[col].dtype == 'bool':\n",
        "            df_feat[col] = df_feat[col].astype(int)\n",
        "\n",
        "    print(f\"Total features created: {len(df_feat.columns)}\")\n",
        "    return df_feat\n",
        "\n",
        "# Apply feature engineering\n",
        "print(\"\\n=== CREATING FEATURES ===\")\n",
        "demand_features = create_comprehensive_features(demand)\n",
        "print(f\"Original columns: {len(demand.columns)}\")\n",
        "print(f\"Enhanced features: {len(demand_features.columns)}\")"
      ],
      "metadata": {
        "id": "1D6Pyawkfk28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_comprehensive_eda(df, sample_year=2023):\n",
        "    \"\"\"\n",
        "    Perform comprehensive exploratory data analysis\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== EXPLORATORY DATA ANALYSIS ===\\n\")\n",
        "\n",
        "    # Basic statistics\n",
        "    print(\"1. BASIC STATISTICS:\")\n",
        "    print(\"-\" * 50)\n",
        "    stats_df = pd.DataFrame({\n",
        "        'Mean': df[['england_wales_demand', 'net_demand', 'embedded_total_generation']].mean(),\n",
        "        'Std': df[['england_wales_demand', 'net_demand', 'embedded_total_generation']].std(),\n",
        "        'Min': df[['england_wales_demand', 'net_demand', 'embedded_total_generation']].min(),\n",
        "        'Max': df[['england_wales_demand', 'net_demand', 'embedded_total_generation']].max(),\n",
        "        'Median': df[['england_wales_demand', 'net_demand', 'embedded_total_generation']].median()\n",
        "    })\n",
        "    print(stats_df.round(2))\n",
        "\n",
        "    # Create visualizations\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "    # 1. Time series of demand (last year for clarity)\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    last_year = df.loc[f'{sample_year}-01-01':f'{sample_year}-12-31']\n",
        "    ax1.plot(last_year.index, last_year['england_wales_demand'],\n",
        "             linewidth=0.5, alpha=0.7, label='Demand')\n",
        "    ax1.plot(last_year.index, last_year['net_demand'],\n",
        "             linewidth=0.5, alpha=0.7, label='Net Demand', color='orange')\n",
        "    ax1.set_title(f'Demand vs Net Demand ({sample_year})')\n",
        "    ax1.set_ylabel('MW')\n",
        "    ax1.legend(loc='upper right')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Average daily profile\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    daily_profile = df.groupby(['hour', 'minute'])['england_wales_demand'].mean().reset_index()\n",
        "    daily_profile['time'] = daily_profile['hour'] + daily_profile['minute']/60\n",
        "    ax2.plot(daily_profile['time'], daily_profile['england_wales_demand'], linewidth=2)\n",
        "    ax2.fill_between(daily_profile['time'],\n",
        "                     daily_profile['england_wales_demand'] * 0.95,\n",
        "                     daily_profile['england_wales_demand'] * 1.05,\n",
        "                     alpha=0.3)\n",
        "    ax2.set_title('Average Daily Demand Profile')\n",
        "    ax2.set_xlabel('Hour of Day')\n",
        "    ax2.set_ylabel('Demand (MW)')\n",
        "    ax2.set_xticks(range(0, 25, 4))\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Monthly patterns\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    monthly_avg = df.groupby(['year', 'month'])['england_wales_demand'].mean().unstack()\n",
        "    monthly_avg.T.plot(ax=ax3, alpha=0.7, linewidth=1)\n",
        "    ax3.set_title('Monthly Average Demand by Year')\n",
        "    ax3.set_xlabel('Month')\n",
        "    ax3.set_ylabel('Average Demand (MW)')\n",
        "    ax3.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Weekly heatmap\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    weekly_heatmap = df.groupby(['day_of_week', 'hour'])['england_wales_demand'].mean().unstack()\n",
        "    im = ax4.imshow(weekly_heatmap, aspect='auto', cmap='YlOrRd')\n",
        "    ax4.set_title('Weekly Demand Pattern Heatmap')\n",
        "    ax4.set_xlabel('Hour of Day')\n",
        "    ax4.set_ylabel('Day of Week')\n",
        "    ax4.set_xticks(range(0, 24, 4))\n",
        "    ax4.set_xticklabels([f'{h:02d}:00' for h in range(0, 24, 4)])\n",
        "    ax4.set_yticks(range(7))\n",
        "    ax4.set_yticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "    plt.colorbar(im, ax=ax4, label='Demand (MW)')\n",
        "\n",
        "    # 5. Embedded generation growth\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    # Ensure only numeric columns are selected for resample().mean()\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    monthly_gen = df[numeric_cols].resample('M').mean()\n",
        "    ax5.plot(monthly_gen.index, monthly_gen['embedded_wind_generation'],\n",
        "             label='Wind', linewidth=2)\n",
        "    ax5.plot(monthly_gen.index, monthly_gen['embedded_solar_generation'],\n",
        "             label='Solar', linewidth=2)\n",
        "    ax5.set_title('Embedded Generation Growth')\n",
        "    ax5.set_xlabel('Year')\n",
        "    ax5.set_ylabel('Average Generation (MW)')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Demand distribution\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    ax6.hist(df['england_wales_demand'], bins=100, edgecolor='black', alpha=0.7)\n",
        "    ax6.axvline(df['england_wales_demand'].mean(), color='red',\n",
        "                linestyle='--', linewidth=2, label=f'Mean: {df[\"england_wales_demand\"].mean():.0f} MW')\n",
        "    ax6.axvline(df['england_wales_demand'].median(), color='green',\n",
        "                linestyle='--', linewidth=2, label=f'Median: {df[\"england_wales_demand\"].median():.0f} MW')\n",
        "    ax6.set_title('Demand Distribution')\n",
        "    ax6.set_xlabel('Demand (MW)')\n",
        "    ax6.set_ylabel('Frequency')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # 7. Interconnector usage over time\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    interconnector_cols = [col for col in df.columns if '_flow' in col and 'total' not in col]\n",
        "    # Ensure only numeric columns are selected for resample().sum()\n",
        "    numeric_interconnector_cols = [col for col in interconnector_cols if col in df.select_dtypes(include=np.number).columns]\n",
        "    yearly_interconnectors = df[numeric_interconnector_cols].resample('Y').sum()\n",
        "    yearly_interconnectors.plot.area(ax=ax7, alpha=0.7)\n",
        "    ax7.set_title('Yearly Interconnector Flows')\n",
        "    ax7.set_xlabel('Year')\n",
        "    ax7.set_ylabel('Total Flow (GWh equivalent)')\n",
        "    ax7.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # 8. Seasonal patterns\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    # Convert season from dummy to numerical if needed for plotting\n",
        "    # Assuming season is handled by dummy variables or numerical encoding now\n",
        "    if 'season_Winter' in df.columns and 'season_Summer' in df.columns: # Check for dummy vars\n",
        "        # Plotting the average demand for each season based on dummy variables\n",
        "        # This requires reconstructing the season from dummies or using the numerical 'season' column if available\n",
        "        # Since create_comprehensive_features now returns 'season' as numeric (1-4),\n",
        "        # we can use that directly.\n",
        "        season_map = {1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Autumn'}\n",
        "        seasonal_avg = df.groupby(['season', 'hour'])['england_wales_demand'].mean().unstack(level=0)\n",
        "        if not seasonal_avg.empty:\n",
        "            seasonal_avg.rename(columns=season_map, inplace=True)\n",
        "            seasonal_avg.plot(ax=ax8, linewidth=2)\n",
        "        else:\n",
        "            ax8.text(0.5, 0.5, 'No seasonal data to plot', horizontalalignment='center', verticalalignment='center', transform=ax8.transAxes)\n",
        "\n",
        "    else:\n",
        "        ax8.text(0.5, 0.5, 'Seasonal dummy variables not found or season column is missing', horizontalalignment='center', verticalalignment='center', transform=ax8.transAxes)\n",
        "\n",
        "    ax8.set_title('Seasonal Demand Patterns')\n",
        "    ax8.set_xlabel('Hour of Day')\n",
        "    ax8.set_ylabel('Demand (MW)')\n",
        "    ax8.legend(title='Season')\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "\n",
        "    # 9. Holiday vs non-holiday comparison\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    holiday_comparison = df.groupby(['is_holiday', 'hour'])['england_wales_demand'].mean().unstack(level=0)\n",
        "    if not holiday_comparison.empty:\n",
        "        holiday_comparison.columns = ['Non-Holiday', 'Holiday']\n",
        "        holiday_comparison.plot(ax=ax9, linewidth=2)\n",
        "    else:\n",
        "        ax9.text(0.5, 0.5, 'No holiday data to plot', horizontalalignment='center', verticalalignment='center', transform=ax9.transAxes)\n",
        "    ax9.set_title('Holiday vs Non-Holiday Demand Patterns')\n",
        "    ax9.set_xlabel('Hour of Day')\n",
        "    ax9.set_ylabel('Demand (MW)')\n",
        "    ax9.legend()\n",
        "    ax9.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print additional insights\n",
        "    print(\"\\n2. KEY INSIGHTS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Peak demand analysis\n",
        "    peak_demand = df['england_wales_demand'].max()\n",
        "    peak_time = df['england_wales_demand'].idxmax()\n",
        "    print(f\"Peak Demand: {peak_demand:,.0f} MW at {peak_time}\")\n",
        "\n",
        "    # Minimum demand\n",
        "    min_demand = df['england_wales_demand'].min()\n",
        "    min_time = df['england_wales_demand'].idxmin()\n",
        "    print(f\"Minimum Demand: {min_demand:,.0f} MW at {min_time}\")\n",
        "\n",
        "    # Demand variability\n",
        "    daily_peak = df.resample('D')['england_wales_demand'].max().mean()\n",
        "    daily_min = df.resample('D')['england_wales_demand'].min().mean()\n",
        "    print(f\"Average Daily Peak: {daily_peak:,.0f} MW\")\n",
        "    print(f\"Average Daily Minimum: {daily_min:,.0f} MW\")\n",
        "    print(f\"Average Daily Range: {daily_peak - daily_min:,.0f} MW\")\n",
        "\n",
        "    # Renewable penetration\n",
        "    max_solar_pen = df['solar_penetration'].max() * 100 if 'solar_penetration' in df.columns else 0\n",
        "    max_wind_pen = df['wind_penetration'].max() * 100 if 'wind_penetration' in df.columns else 0\n",
        "    print(f\"Maximum Solar Penetration: {max_solar_pen:.1f}%\")\n",
        "    print(f\"Maximum Wind Penetration: {max_wind_pen:.1f}%\")\n",
        "\n",
        "    # Growth trends\n",
        "    first_year_demand = df.loc['2009']['england_wales_demand'].mean()\n",
        "    last_year_demand = df.loc['2023']['england_wales_demand'].mean()\n",
        "    growth = ((last_year_demand - first_year_demand) / first_year_demand) * 100\n",
        "    print(f\"\\nDemand Growth 2009-2023: {growth:.1f}%\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Perform EDA\n",
        "fig = perform_comprehensive_eda(demand_features)"
      ],
      "metadata": {
        "id": "tWw3tXDohkbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def train_evaluate_models(X_train_scaled, X_test_scaled, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple forecasting models\n",
        "    \"\"\"\n",
        "    print(\"\\n=== TRAINING MODELS ===\\n\")\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'XGBoost': xgb.XGBRegressor(\n",
        "            n_estimators=200,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbosity=0\n",
        "        ),\n",
        "        'Random Forest': RandomForestRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        ),\n",
        "        'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
        "        'Lasso Regression': Lasso(alpha=0.01, random_state=42, max_iter=5000),\n",
        "        'Neural Network': MLPRegressor(\n",
        "            hidden_layer_sizes=(100, 50),\n",
        "            activation='relu',\n",
        "            solver='adam',\n",
        "            max_iter=500,\n",
        "            random_state=42\n",
        "        )\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    predictions = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Calculate accuracy within tolerances\n",
        "        tolerance_5pct = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.05) * 100\n",
        "        tolerance_2pct = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.02) * 100\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'R2': r2,\n",
        "            'Within 5%': tolerance_5pct,\n",
        "            'Within 2%': tolerance_2pct,\n",
        "            'model': model\n",
        "        }\n",
        "\n",
        "        predictions[name] = y_pred\n",
        "\n",
        "        print(f\"  MAE: {mae:,.0f} MW | RMSE: {rmse:,.0f} MW | MAPE: {mape:.2f}% | R²: {r2:.3f}\")\n",
        "        print(f\"  Within 5%: {tolerance_5pct:.1f}% | Within 2%: {tolerance_2pct:.1f}%\")\n",
        "        print()\n",
        "\n",
        "    # Create comparison DataFrame\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    results_df = results_df[['MAE', 'RMSE', 'MAPE', 'R2', 'Within 5%', 'Within 2%']]\n",
        "\n",
        "    print(\"\\n=== MODEL COMPARISON ===\")\n",
        "    print(\"-\" * 80)\n",
        "    print(results_df.round(3))\n",
        "\n",
        "    return results, predictions, results_df\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "TARGET = 'england_wales_demand'\n",
        "X = demand_features.drop(columns=[TARGET])\n",
        "y = demand_features[TARGET]\n",
        "\n",
        "# Define the split point for time series\n",
        "split_date = '2023-01-01'\n",
        "X_train = X.loc[X.index < split_date]\n",
        "y_train = y.loc[y.index < split_date]\n",
        "X_test = X.loc[X.index >= split_date]\n",
        "y_test = y.loc[y.index >= split_date]\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert scaled arrays back to DataFrames for model compatibility and feature tracking\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "# Train and evaluate models\n",
        "results, predictions, results_df = train_evaluate_models(\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC11gi4DhoY0",
        "outputId": "eb7ccc15-d206-40f3-beb6-3bb7fda21c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (245424, 72), y_train shape: (245424,)\n",
            "X_test shape: (33840, 72), y_test shape: (33840,)\n",
            "\n",
            "=== TRAINING MODELS ===\n",
            "\n",
            "Training XGBoost...\n",
            "  MAE: 149 MW | RMSE: 201 MW | MAPE: 0.65% | R²: 0.999\n",
            "  Within 5%: 99.8% | Within 2%: 97.6%\n",
            "\n",
            "Training Random Forest...\n",
            "  MAE: 205 MW | RMSE: 274 MW | MAPE: 0.88% | R²: 0.997\n",
            "  Within 5%: 99.8% | Within 2%: 91.9%\n",
            "\n",
            "Training Gradient Boosting...\n",
            "  MAE: 194 MW | RMSE: 258 MW | MAPE: 0.84% | R²: 0.998\n",
            "  Within 5%: 99.6% | Within 2%: 93.4%\n",
            "\n",
            "Training Ridge Regression...\n",
            "  MAE: 0 MW | RMSE: 0 MW | MAPE: 0.00% | R²: 1.000\n",
            "  Within 5%: 100.0% | Within 2%: 100.0%\n",
            "\n",
            "Training Lasso Regression...\n",
            "  MAE: 248 MW | RMSE: 409 MW | MAPE: 1.06% | R²: 0.994\n",
            "  Within 5%: 99.3% | Within 2%: 73.3%\n",
            "\n",
            "Training Neural Network...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_forecast_results(y_test, predictions, results_df, sample_days=7):\n",
        "    \"\"\"\n",
        "    Visualize forecast results and model performance\n",
        "    \"\"\"\n",
        "    print(\"\\n=== VISUALIZING RESULTS ===\\n\")\n",
        "\n",
        "    # Convert predictions to DataFrame\n",
        "    pred_df = pd.DataFrame(predictions, index=y_test.index)\n",
        "    pred_df['Actual'] = y_test\n",
        "\n",
        "    # Sample a week for detailed visualization\n",
        "    sample_start = pred_df.index[0]\n",
        "    sample_end = sample_start + pd.Timedelta(days=sample_days)\n",
        "    sample_data = pred_df.loc[sample_start:sample_end]\n",
        "\n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
        "\n",
        "    # 1. Time series comparison (best model)\n",
        "    best_model = results_df['MAPE'].idxmin()\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(sample_data.index, sample_data['Actual'],\n",
        "             label='Actual', linewidth=2, color='black')\n",
        "    ax1.plot(sample_data.index, sample_data[best_model],\n",
        "             label=f'{best_model} Prediction', linewidth=1.5,\n",
        "             linestyle='--', alpha=0.8)\n",
        "    ax1.fill_between(sample_data.index,\n",
        "                     sample_data['Actual'] * 0.97,\n",
        "                     sample_data['Actual'] * 1.03,\n",
        "                     alpha=0.2, color='gray', label='±3% Band')\n",
        "    ax1.set_title(f'Best Model ({best_model}) Forecast vs Actual\\n(Sample Week)')\n",
        "    ax1.set_ylabel('Demand (MW)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
        "\n",
        "    # 2. Error distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    errors = {}\n",
        "    for model in predictions.keys():\n",
        "        errors[model] = (predictions[model] - y_test) / y_test * 100\n",
        "\n",
        "    error_df = pd.DataFrame(errors)\n",
        "    error_df.plot.kde(ax=ax2, linewidth=2)\n",
        "    ax2.axvline(0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    ax2.set_title('Error Distribution by Model')\n",
        "    ax2.set_xlabel('Percentage Error (%)')\n",
        "    ax2.set_ylabel('Density')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "\n",
        "    # 3. Scatter plot of predictions vs actual\n",
        "    ax3 = axes[1, 0]\n",
        "    for model in ['XGBoost', 'Random Forest', 'Gradient Boosting']:\n",
        "        if model in predictions:\n",
        "            ax3.scatter(y_test, predictions[model],\n",
        "                       alpha=0.1, s=10, label=model)\n",
        "    ax3.plot([y_test.min(), y_test.max()],\n",
        "             [y_test.min(), y_test.max()],\n",
        "             'k--', linewidth=2, label='Perfect Prediction')\n",
        "    ax3.set_title('Predictions vs Actual (All Test Data)')\n",
        "    ax3.set_xlabel('Actual Demand (MW)')\n",
        "    ax3.set_ylabel('Predicted Demand (MW)')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Model performance comparison\n",
        "    ax4 = axes[1, 1]\n",
        "    metrics_to_plot = ['MAE', 'RMSE', 'MAPE']\n",
        "    metrics_df = results_df[metrics_to_plot].copy()\n",
        "    # Normalize for better visualization\n",
        "    metrics_normalized = metrics_df / metrics_df.max()\n",
        "\n",
        "    x = np.arange(len(metrics_df.index))\n",
        "    width = 0.25\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        offset = width * (i - 1)\n",
        "        ax4.bar(x + offset, metrics_normalized[metric],\n",
        "                width, label=metric)\n",
        "\n",
        "    ax4.set_title('Model Performance Comparison (Normalized)')\n",
        "    ax4.set_xlabel('Model')\n",
        "    ax4.set_ylabel('Normalized Metric (Lower is Better)')\n",
        "    ax4.set_xticks(x)\n",
        "    ax4.set_xticklabels(metrics_df.index, rotation=45)\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # 5. Feature importance (XGBoost)\n",
        "    ax5 = axes[2, 0]\n",
        "    if 'XGBoost' in results:\n",
        "        xgb_model = results['XGBoost']['model']\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': features,\n",
        "            'importance': xgb_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=True).tail(15)\n",
        "\n",
        "        ax5.barh(feature_importance['feature'],\n",
        "                feature_importance['importance'])\n",
        "        ax5.set_title('Top 15 Feature Importances (XGBoost)')\n",
        "        ax5.set_xlabel('Importance Score')\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Prediction error by time of day\n",
        "    ax6 = axes[2, 1]\n",
        "    error_by_hour = pd.DataFrame({\n",
        "        'hour': y_test.index.hour,\n",
        "        'error_pct': np.abs((predictions[best_model] - y_test) / y_test * 100)\n",
        "    })\n",
        "    hourly_error = error_by_hour.groupby('hour')['error_pct'].mean()\n",
        "\n",
        "    ax6.bar(hourly_error.index, hourly_error.values)\n",
        "    ax6.set_title(f'Average Prediction Error by Hour of Day\\n({best_model} Model)')\n",
        "    ax6.set_xlabel('Hour of Day')\n",
        "    ax6.set_ylabel('Mean Absolute Percentage Error (%)')\n",
        "    ax6.set_xticks(range(0, 24, 2))\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed performance analysis\n",
        "    print(\"DETAILED PERFORMANCE ANALYSIS:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_overall"
      ],
      "metadata": {
        "id": "TkBjMVHAiAPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #========================================"
      ],
      "metadata": {
        "id": "xkZE9JETiOm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== HANDLING DUPLICATES ===\\n\")\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_mask = demand.index.duplicated(keep=False)\n",
        "duplicates = demand[duplicate_mask]\n",
        "\n",
        "print(f\"Found {len(duplicates)} duplicate timestamps\")\n",
        "if len(duplicates) > 0:\n",
        "    print(\"\\nSample duplicates:\")\n",
        "    print(duplicates.head(10))\n",
        "\n",
        "    # Remove duplicates, keeping the first occurrence\n",
        "    demand = demand[~demand.index.duplicated(keep='first')]\n",
        "    print(f\"\\nData shape after removing duplicates: {demand.shape}\")"
      ],
      "metadata": {
        "id": "w6pYWVGhKHOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_comprehensive_features_fixed(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive features for electricity demand forecasting\n",
        "    \"\"\"\n",
        "    df_feat = df.copy()\n",
        "\n",
        "    # 1. Temporal Features\n",
        "    df_feat['hour'] = df_feat.index.hour\n",
        "    df_feat['minute'] = df_feat.index.minute\n",
        "    df_feat['half_hour_period'] = df_feat['hour'] * 2 + (df_feat['minute'] // 30)\n",
        "    df_feat['day_of_week'] = df_feat.index.dayofweek\n",
        "    df_feat['day_of_month'] = df_feat.index.day\n",
        "    df_feat['month'] = df_feat.index.month\n",
        "    df_feat['quarter'] = df_feat.index.quarter\n",
        "    df_feat['year'] = df_feat.index.year\n",
        "    df_feat['day_of_year'] = df_feat.index.dayofyear\n",
        "    df_feat['week_of_year'] = df_feat.index.isocalendar().week\n",
        "\n",
        "    # 2. Calendar Features\n",
        "    df_feat['is_weekend'] = (df_feat['day_of_week'] >= 5).astype(int)\n",
        "    df_feat['is_weekday'] = (df_feat['day_of_week'] < 5).astype(int)\n",
        "    df_feat['is_working_day'] = ((df_feat['day_of_week'] < 5) & (df_feat['is_holiday'] == 0)).astype(int)\n",
        "\n",
        "    # Special periods\n",
        "    df_feat['is_morning_peak'] = ((df_feat['hour'] >= 7) & (df_feat['hour'] <= 10)).astype(int)\n",
        "    df_feat['is_evening_peak'] = ((df_feat['hour'] >= 16) & (df_feat['hour'] <= 20)).astype(int)\n",
        "    df_feat['is_overnight'] = ((df_feat['hour'] >= 0) & (df_feat['hour'] <= 5)).astype(int)\n",
        "\n",
        "    # 3. Calculate Net Demand\n",
        "    df_feat['embedded_total_generation'] = df_feat['embedded_wind_generation'] + df_feat['embedded_solar_generation']\n",
        "    df_feat['net_demand'] = df_feat['england_wales_demand'] - df_feat['embedded_total_generation']\n",
        "\n",
        "    # 4. Capacity Factors\n",
        "    df_feat['wind_capacity_factor'] = np.where(\n",
        "        df_feat['embedded_wind_capacity'] > 0,\n",
        "        df_feat['embedded_wind_generation'] / df_feat['embedded_wind_capacity'],\n",
        "        0\n",
        "    )\n",
        "    df_feat['solar_capacity_factor'] = np.where(\n",
        "        df_feat['embedded_solar_capacity'] > 0,\n",
        "        df_feat['embedded_solar_generation'] / df_feat['embedded_solar_capacity'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 5. Interconnector Analysis\n",
        "    interconnector_cols = [col for col in df.columns if '_flow' in col]\n",
        "\n",
        "    # Fill missing values in interconnectors\n",
        "    for col in interconnector_cols:\n",
        "        # Use forward fill and backward fill (updated syntax)\n",
        "        df_feat[col] = df_feat[col].ffill().bfill().fillna(0)\n",
        "\n",
        "    # Calculate interconnector statistics\n",
        "    df_feat['total_interconnector_flow'] = df_feat[interconnector_cols].sum(axis=1)\n",
        "    df_feat['interconnector_imports'] = df_feat[interconnector_cols].clip(lower=0).sum(axis=1)\n",
        "    df_feat['interconnector_exports'] = df_feat[interconnector_cols].clip(upper=0).abs().sum(axis=1)\n",
        "\n",
        "    # 6. Cyclical Encoding for Time Features\n",
        "    df_feat['hour_sin'] = np.sin(2 * np.pi * df_feat['hour'] / 24)\n",
        "    df_feat['hour_cos'] = np.cos(2 * np.pi * df_feat['hour'] / 24)\n",
        "    df_feat['day_of_week_sin'] = np.sin(2 * np.pi * df_feat['day_of_week'] / 7)\n",
        "    df_feat['day_of_week_cos'] = np.cos(2 * np.pi * df_feat['day_of_week'] / 7)\n",
        "    df_feat['month_sin'] = np.sin(2 * np.pi * df_feat['month'] / 12)\n",
        "    df_feat['month_cos'] = np.cos(2 * np.pi * df_feat['month'] / 12)\n",
        "\n",
        "    # 7. Lag Features (Autoregressive)\n",
        "    df_feat['demand_lag_1'] = df_feat['england_wales_demand'].shift(1)\n",
        "    df_feat['demand_lag_2'] = df_feat['england_wales_demand'].shift(2)\n",
        "    df_feat['demand_lag_48'] = df_feat['england_wales_demand'].shift(48)\n",
        "    df_feat['demand_lag_336'] = df_feat['england_wales_demand'].shift(336)\n",
        "    df_feat['net_demand_lag_48'] = df_feat['net_demand'].shift(48)\n",
        "\n",
        "    # 8. Rolling Statistics\n",
        "    df_feat['demand_rolling_24h_mean'] = df_feat['england_wales_demand'].rolling(window=48, min_periods=1).mean()\n",
        "    df_feat['demand_rolling_24h_std'] = df_feat['england_wales_demand'].rolling(window=48, min_periods=1).std()\n",
        "    df_feat['demand_rolling_7d_mean'] = df_feat['england_wales_demand'].rolling(window=336, min_periods=1).mean()\n",
        "    df_feat['demand_rolling_7d_std'] = df_feat['england_wales_demand'].rolling(window=336, min_periods=1).std()\n",
        "\n",
        "    # 9. Rate of Change\n",
        "    df_feat['demand_change_1h'] = df_feat['england_wales_demand'].diff(2)\n",
        "    df_feat['demand_change_24h'] = df_feat['england_wales_demand'].diff(48)\n",
        "\n",
        "    # 10. Penetration Rates\n",
        "    # Avoid division by zero\n",
        "    df_feat['wind_penetration'] = np.where(\n",
        "        df_feat['england_wales_demand'] > 0,\n",
        "        df_feat['embedded_wind_generation'] / df_feat['england_wales_demand'],\n",
        "        0\n",
        "    )\n",
        "    df_feat['solar_penetration'] = np.where(\n",
        "        df_feat['england_wales_demand'] > 0,\n",
        "        df_feat['embedded_solar_generation'] / df_feat['england_wales_demand'],\n",
        "        0\n",
        "    )\n",
        "    df_feat['total_renewable_penetration'] = np.where(\n",
        "        df_feat['england_wales_demand'] > 0,\n",
        "        (df_feat['embedded_wind_generation'] + df_feat['embedded_solar_generation']) / df_feat['england_wales_demand'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 11. Season Indicators (as numeric, not categorical for now)\n",
        "    # 1: Winter, 2: Spring, 3: Summer, 4: Autumn\n",
        "    def get_season_numeric(month):\n",
        "        if month in [12, 1, 2]:\n",
        "            return 1  # Winter\n",
        "        elif month in [3, 4, 5]:\n",
        "            return 2  # Spring\n",
        "        elif month in [6, 7, 8]:\n",
        "            return 3  # Summer\n",
        "        else:\n",
        "            return 4  # Autumn\n",
        "\n",
        "    df_feat['season'] = df_feat['month'].apply(get_season_numeric)\n",
        "\n",
        "    # 12. Time of Day Categories (as numeric)\n",
        "    def get_time_of_day_numeric(hour):\n",
        "        if 0 <= hour < 6:\n",
        "            return 1  # Night\n",
        "        elif 6 <= hour < 9:\n",
        "            return 2  # Morning Peak\n",
        "        elif 9 <= hour < 16:\n",
        "            return 3  # Day\n",
        "        elif 16 <= hour < 19:\n",
        "            return 4  # Evening Peak\n",
        "        else:\n",
        "            return 5  # Evening\n",
        "\n",
        "    df_feat['time_of_day'] = df_feat['hour'].apply(get_time_of_day_numeric)\n",
        "\n",
        "    # Fill any remaining NaN values\n",
        "    df_feat = df_feat.ffill().bfill().fillna(0)\n",
        "\n",
        "    print(f\"Total features created: {len(df_feat.columns)}\")\n",
        "    return df_feat\n",
        "\n",
        "# Apply fixed feature engineering\n",
        "print(\"\\n=== CREATING FEATURES (FIXED) ===\")\n",
        "demand_features = create_comprehensive_features_fixed(demand)\n",
        "print(f\"Original columns: {len(demand.columns)}\")\n",
        "print(f\"Enhanced features: {len(demand_features.columns)}\")\n",
        "\n",
        "# Display the new features\n",
        "print(\"\\nNew features created:\")\n",
        "new_features = [col for col in demand_features.columns if col not in demand.columns]\n",
        "print(f\"Total new features: {len(new_features)}\")\n",
        "print(\"Sample of new features:\", new_features[:20])"
      ],
      "metadata": {
        "id": "TVXfvJNpLaMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is removed as its functionality is now integrated into cell LmCcJV-0KKI1."
      ],
      "metadata": {
        "id": "TvDK7TXNLaP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_features(df):\n",
        "    df_feat = df.copy()\n",
        "\n",
        "    # --- Temporal\n",
        "    df_feat[\"hour\"] = df_feat.index.hour\n",
        "    df_feat[\"minute\"] = df_feat.index.minute\n",
        "    df_feat[\"half_hour_period\"] = df_feat[\"hour\"] * 2 + (df_feat[\"minute\"] // 30)\n",
        "    df_feat[\"day_of_week\"] = df_feat.index.dayofweek\n",
        "    df_feat[\"day_of_month\"] = df_feat.index.day\n",
        "    df_feat[\"month\"] = df_feat.index.month\n",
        "    df_feat[\"quarter\"] = df_feat.index.quarter\n",
        "    df_feat[\"year\"] = df_feat.index.year\n",
        "    df_feat[\"day_of_year\"] = df_feat.index.dayofyear\n",
        "    df_feat[\"week_of_year\"] = df_feat.index.isocalendar().week.astype(int)\n",
        "\n",
        "    # --- Calendar\n",
        "    df_feat[\"is_weekend\"] = (df_feat[\"day_of_week\"] >= 5).astype(int)\n",
        "    df_feat[\"is_working_day\"] = ((df_feat[\"day_of_week\"] < 5) & (df_feat[\"is_holiday\"] == 0)).astype(int)\n",
        "\n",
        "    df_feat[\"is_morning_peak\"] = ((df_feat[\"hour\"] >= 7) & (df_feat[\"hour\"] <= 10)).astype(int)\n",
        "    df_feat[\"is_evening_peak\"] = ((df_feat[\"hour\"] >= 16) & (df_feat[\"hour\"] <= 20)).astype(int)\n",
        "    df_feat[\"is_overnight\"] = ((df_feat[\"hour\"] >= 0) & (df_feat[\"hour\"] <= 5)).astype(int)\n",
        "\n",
        "    # --- Net demand\n",
        "    df_feat[\"embedded_total_generation\"] = (\n",
        "        df_feat[\"embedded_wind_generation\"].fillna(0) + df_feat[\"embedded_solar_generation\"].fillna(0)\n",
        "    )\n",
        "    df_feat[\"net_demand\"] = df_feat[\"england_wales_demand\"] - df_feat[\"embedded_total_generation\"]\n",
        "\n",
        "    # --- Capacity factors\n",
        "    df_feat[\"wind_capacity_factor\"] = np.where(\n",
        "        df_feat[\"embedded_wind_capacity\"] > 0,\n",
        "        df_feat[\"embedded_wind_generation\"] / df_feat[\"embedded_wind_capacity\"],\n",
        "        0\n",
        "    )\n",
        "    df_feat[\"solar_capacity_factor\"] = np.where(\n",
        "        df_feat[\"embedded_solar_capacity\"] > 0,\n",
        "        df_feat[\"embedded_solar_generation\"] / df_feat[\"embedded_solar_capacity\"],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # --- Interconnectors\n",
        "    interconnector_cols = [c for c in df_feat.columns if c.endswith(\"_flow\")]\n",
        "    for c in interconnector_cols:\n",
        "        df_feat[c] = df_feat[c].ffill().bfill().fillna(0)\n",
        "\n",
        "    df_feat[\"total_interconnector_flow\"] = df_feat[interconnector_cols].sum(axis=1)\n",
        "    df_feat[\"interconnector_imports\"] = df_feat[interconnector_cols].clip(lower=0).sum(axis=1)\n",
        "    df_feat[\"interconnector_exports\"] = df_feat[interconnector_cols].clip(upper=0).abs().sum(axis=1)\n",
        "\n",
        "    # --- Cyclical encodings\n",
        "    df_feat[\"hour_sin\"] = np.sin(2 * np.pi * df_feat[\"hour\"] / 24)\n",
        "    df_feat[\"hour_cos\"] = np.cos(2 * np.pi * df_feat[\"hour\"] / 24)\n",
        "    df_feat[\"day_of_week_sin\"] = np.sin(2 * np.pi * df_feat[\"day_of_week\"] / 7)\n",
        "    df_feat[\"day_of_week_cos\"] = np.cos(2 * np.pi * df_feat[\"day_of_week\"] / 7)\n",
        "    df_feat[\"month_sin\"] = np.sin(2 * np.pi * df_feat[\"month\"] / 12)\n",
        "    df_feat[\"month_cos\"] = np.cos(2 * np.pi * df_feat[\"month\"] / 12)\n",
        "\n",
        "    # --- Lags (half-hourly)\n",
        "    df_feat[\"demand_lag_1\"] = df_feat[\"england_wales_demand\"].shift(1)\n",
        "    df_feat[\"demand_lag_48\"] = df_feat[\"england_wales_demand\"].shift(48)     # 24h\n",
        "    df_feat[\"demand_lag_336\"] = df_feat[\"england_wales_demand\"].shift(336)   # 7d\n",
        "    df_feat[\"net_demand_lag_48\"] = df_feat[\"net_demand\"].shift(48)\n",
        "\n",
        "    # --- Rolling stats (LEAKAGE-SAFE: shift by 1)\n",
        "    df_feat[\"demand_roll_24h_mean\"] = df_feat[\"england_wales_demand\"].rolling(48).mean().shift(1)\n",
        "    df_feat[\"demand_roll_7d_mean\"] = df_feat[\"england_wales_demand\"].rolling(336).mean().shift(1)\n",
        "\n",
        "    # --- Change features (optional leakage-safe)\n",
        "    df_feat[\"demand_change_1h\"] = df_feat[\"england_wales_demand\"].diff(2).shift(1)\n",
        "    df_feat[\"demand_change_24h\"] = df_feat[\"england_wales_demand\"].diff(48).shift(1)\n",
        "\n",
        "    # --- Penetration\n",
        "    demand_safe = df_feat[\"england_wales_demand\"].replace(0, np.nan)\n",
        "    df_feat[\"wind_penetration\"] = (df_feat[\"embedded_wind_generation\"] / demand_safe).fillna(0)\n",
        "    df_feat[\"solar_penetration\"] = (df_feat[\"embedded_solar_generation\"] / demand_safe).fillna(0)\n",
        "\n",
        "    # Fill remaining NaNs after lags/rollings\n",
        "    df_feat = df_feat.dropna()\n",
        "\n",
        "    return df_feat\n"
      ],
      "metadata": {
        "id": "GQWB49XKLaV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "jcF6D0hZWJbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# !pip install optuna # Commenting out as it's installed in a separate cell now\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 2000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "    }\n",
        "\n",
        "    # Time-series CV (walk-forward)\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    fold_scores = []\n",
        "    for train_idx, valid_idx in tscv.split(X_train_scaled, y_train):\n",
        "        X_tr, X_val = X_train_scaled.iloc[train_idx], X_train_scaled.iloc[valid_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
        "\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "\n",
        "        model.fit(\n",
        "            X_tr,\n",
        "            y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False,\n",
        "            early_stopping_rounds=50,\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "\n",
        "        # Choose ONE metric to optimize\n",
        "        mape = mean_absolute_percentage_error(y_val, preds)\n",
        "        fold_scores.append(mape)\n",
        "\n",
        "    return float(np.mean(fold_scores))\n",
        "\n",
        "# Create the Optuna study and optimize\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "# Print the best trial's parameters and the best value achieved\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value (MAPE): {trial.value:.6f}\")\n",
        "print(\"  Params:\")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ],
      "metadata": {
        "id": "WIT50liwLaZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 2000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "    }\n",
        "\n",
        "    # Time-series CV (walk-forward)\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    fold_scores = []\n",
        "    for train_idx, valid_idx in tscv.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
        "\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False,\n",
        "            early_stopping_rounds=50\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "\n",
        "        # Choose ONE metric to optimize\n",
        "        mape = mean_absolute_percentage_error(y_val, preds)\n",
        "        fold_scores.append(mape)\n",
        "\n",
        "    return float(np.mean(fold_scores))\n",
        "\n",
        "# Create the Optuna study and optimize\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "# Print the best trial's parameters and the best value achieved\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value (MAPE): {trial.value:.6f}\")\n",
        "print(\"  Params:\")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "JI8_CKTqV6x_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}